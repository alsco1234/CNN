<aside>
💡 Image에 Tranformer적용하기 위해 encoder, 특히 Multi-Head Self (dot)Attention를 사용**

</aside>

github 원본 : 
https://github.com/google-research/vision_transformer


Hugging face 코드설명
https://huggingface.co/docs/transformers/model_doc/vit 

Fashion mnist train
https://colab.research.google.com/drive/1Q-js3ugu1Xx-a4oH-KHDUsOmYXjGF4Er?authuser=1#scrollTo=BcK7907n6KOs 

Fashion mnist train 2
https://colab.research.google.com/github/d2l-ai/d2l-pytorch-colab/blob/master/chapter_attention-mechanisms-and-transformers/vision-transformer.ipynb 

Imagenet pretrained된거가지고 한장실험
https://colab.research.google.com/drive/1muZ4QFgVfwALgqmrfOkp7trAvqDemckO?usp=sharing#scrollTo=Ma1EIr7UyCOE 


=====
fashion_mnist : vit
https://colab.research.google.com/drive/1Q-js3ugu1Xx-a4oH-KHDUsOmYXjGF4Er?authuser=1#scrollTo=iErpsHra6KOw 
처음부터학습, 10개레이블, train epoch10

Fashion_mnist: can
https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/tutorials/keras/classification.ipynb?hl=ko&authuser=3 
처음부터학습, 10개레이블, train epoch10