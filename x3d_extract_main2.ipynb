{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HC0cII3jUpN8",
        "outputId": "439982ea-c96f-42ec-acb1-62864fa03f90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sApJkr4q2bwg",
        "outputId": "5c9a6e32-de55-4b2e-9aba-425585adc937"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wX44zCwwUoOe",
        "outputId": "a20b2412-475c-4141-8155-6bda096516af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Extract-Features-X3D'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 15 (delta 4), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (15/15), 10.70 MiB | 5.73 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/longphamkhac/Extract-Features-X3D.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kcg6yxPWt5AD",
        "outputId": "cab48d57-449e-42f1-e0a1-9cc2dc306f9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/facebookresearch/pytorchvideo.git\n",
            "  Cloning https://github.com/facebookresearch/pytorchvideo.git to /tmp/pip-req-build-c3tvxeol\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/pytorchvideo.git /tmp/pip-req-build-c3tvxeol\n",
            "  Resolved https://github.com/facebookresearch/pytorchvideo.git to commit 702f9f42569598c5cce8c5e2dd7e37c3d6c46efd\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 KB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting av\n",
            "  Downloading av-10.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting parameterized\n",
            "  Downloading parameterized-0.8.1-py2.py3-none-any.whl (26 kB)\n",
            "Collecting iopath\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from pytorchvideo==0.1.5) (3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from fvcore->pytorchvideo==0.1.5) (1.21.6)\n",
            "Collecting yacs>=0.1.6\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from fvcore->pytorchvideo==0.1.5) (6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from fvcore->pytorchvideo==0.1.5) (4.64.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.8/dist-packages (from fvcore->pytorchvideo==0.1.5) (2.2.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from fvcore->pytorchvideo==0.1.5) (7.1.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from fvcore->pytorchvideo==0.1.5) (0.8.10)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.8/dist-packages (from iopath->pytorchvideo==0.1.5) (4.4.0)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Building wheels for collected packages: pytorchvideo, fvcore, iopath\n",
            "  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=211012 sha256=0fcc5a4fd8aea8f773f3933aa4f7006db48239081355366ac1a02e855051156e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ftkl3ntf/wheels/5d/b6/bb/9660d1add295846945e1a94e61b64a4cb98ffddfcb7345a9d1\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61431 sha256=7cbc23ff679d5406f2baeaff288cdc87383f6482c55a211e101fc62d2e0eeebe\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/79/07/c0e9367f5b5ea325e246bd73651e8af175fabbef943043b1cc\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31547 sha256=dadfb21b9332b9da46d5deb037f36402d5e2dedebbe9d4e6fb667d769872daed\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/3e/24/0f349c0b2eeb6965903035f3b00dbb5c9bea437b4a2f18d82c\n",
            "Successfully built pytorchvideo fvcore iopath\n",
            "Installing collected packages: parameterized, av, yacs, portalocker, iopath, fvcore, pytorchvideo\n",
            "Successfully installed av-10.0.0 fvcore-0.1.5.post20221221 iopath-0.1.10 parameterized-0.8.1 portalocker-2.7.0 pytorchvideo-0.1.5 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "!pip install \"git+https://github.com/facebookresearch/pytorchvideo.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9M7ws8ePsbVR"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Mostly from torchvision\n",
        "\"\"\"\n",
        "import torch\n",
        "from typing import Iterable\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "\n",
        "def crop(vid, i, j, h, w):\n",
        "    return vid[..., i:(i + h), j:(j + w)]\n",
        "\n",
        "\n",
        "def center_crop(vid, output_size):\n",
        "    h, w = vid.shape[-2:]\n",
        "    th, tw = output_size\n",
        "\n",
        "    i = int(round((h - th) / 2.))\n",
        "    j = int(round((w - tw) / 2.))\n",
        "    return crop(vid, i, j, th, tw)\n",
        "\n",
        "\n",
        "def hflip(vid):\n",
        "    return vid.flip(dims=(-1,))\n",
        "\n",
        "\n",
        "def pad(vid, padding, fill=0, padding_mode=\"constant\"):\n",
        "    # NOTE: don't want to pad on temporal dimension, so let as non-batch\n",
        "    # (4d) before padding. This works as expected\n",
        "    return torch.nn.functional.pad(vid, padding, value=fill, mode=padding_mode)\n",
        "\n",
        "\n",
        "def to_normalized_float_tensor(vid):\n",
        "    return vid.permute(3, 0, 1, 2).to(torch.float32) / 255\n",
        "\n",
        "\n",
        "def normalize(vid, mean, std):\n",
        "    shape = (-1,) + (1,) * (vid.dim() - 1)\n",
        "    mean = torch.as_tensor(mean).reshape(shape)\n",
        "    std = torch.as_tensor(std).reshape(shape)\n",
        "    return (vid - mean) / std\n",
        "\n",
        "\n",
        "# Class interface\n",
        "\n",
        "class RandomCrop(object):\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    @staticmethod\n",
        "    def get_params(vid, output_size):\n",
        "        \"\"\"Get parameters for ``crop`` for a random crop.\n",
        "        \"\"\"\n",
        "        h, w = vid.shape[-2:]\n",
        "        th, tw = output_size\n",
        "        if w == tw and h == th:\n",
        "            return 0, 0, h, w\n",
        "        i = random.randint(0, h - th)\n",
        "        j = random.randint(0, w - tw)\n",
        "        return i, j, th, tw\n",
        "\n",
        "    def __call__(self, vid):\n",
        "        i, j, h, w = self.get_params(vid, self.size)\n",
        "        return crop(vid, i, j, h, w)\n",
        "\n",
        "\n",
        "class CenterCrop(object):\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    def __call__(self, vid):\n",
        "        return center_crop(vid, self.size)\n",
        "\n",
        "\n",
        "class Resize(object):\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    def __call__(self, vid):\n",
        "        # NOTE: for those functions, which generally expect mini-batches, we keep them\n",
        "        # as non-minibatch so that they are applied as if they were 4d (thus image).\n",
        "        # this way, we only apply the transformation in the spatial domain\n",
        "        interpolation = 'bilinear'\n",
        "        # NOTE: using bilinear interpolation because we don't work on minibatches\n",
        "        # at this level\n",
        "        scale = None\n",
        "        if isinstance(self.size, int):\n",
        "            scale = float(self.size) / min(vid.shape[-2:])\n",
        "            size = None\n",
        "        else:\n",
        "            size = self.size\n",
        "        return torch.nn.functional.interpolate(\n",
        "            vid, size=size, scale_factor=scale, mode=interpolation, align_corners=False,\n",
        "            recompute_scale_factor=False\n",
        "        )\n",
        "\n",
        "\n",
        "class ToFloatTensorInZeroOne(object):\n",
        "    def __call__(self, vid):\n",
        "        return to_normalized_float_tensor(vid)\n",
        "\n",
        "\n",
        "class Normalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, vid):\n",
        "        return normalize(vid, self.mean, self.std)\n",
        "\n",
        "\n",
        "class RandomHorizontalFlip(object):\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, vid):\n",
        "        if random.random() < self.p:\n",
        "            return hflip(vid)\n",
        "        return vid\n",
        "\n",
        "\n",
        "class Pad(object):\n",
        "    def __init__(self, padding, fill=0):\n",
        "        self.padding = padding\n",
        "        self.fill = fill\n",
        "\n",
        "    def __call__(self, vid):\n",
        "        return pad(vid, self.padding, self.fill)\n",
        "\n",
        "\n",
        "class TensorCenterCrop(object):\n",
        "\n",
        "    def __init__(self, crop_size: int) -> None:\n",
        "        self.crop_size = crop_size\n",
        "\n",
        "    def __call__(self, tensor: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        H, W = tensor.size(-2), tensor.size(-1)\n",
        "        from_H = ((H - self.crop_size) // 2)\n",
        "        from_W = ((W - self.crop_size) // 2)\n",
        "        to_H = from_H + self.crop_size\n",
        "        to_W = from_W + self.crop_size\n",
        "        return tensor[..., from_H:to_H, from_W:to_W]\n",
        "\n",
        "\n",
        "class ScaleTo1_1(object):\n",
        "\n",
        "    def __call__(self, tensor: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        return (2 * tensor / 255) - 1\n",
        "\n",
        "\n",
        "class PermuteAndUnsqueeze(object):\n",
        "\n",
        "    def __call__(self, tensor: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        return tensor.permute(1, 0, 2, 3).unsqueeze(0)\n",
        "\n",
        "\n",
        "class Clamp(object):\n",
        "\n",
        "    def __init__(self, min_val, max_val) -> None:\n",
        "        self.min_val = min_val\n",
        "        self.max_val = max_val\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        return torch.clamp(tensor, min=self.min_val, max=self.max_val)\n",
        "\n",
        "\n",
        "class ToUInt8(object):\n",
        "\n",
        "    def __call__(self, flow_tensor: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        # preprocessing as in\n",
        "        # https://github.com/deepmind/kinetics-i3d/issues/61#issuecomment-506727158\n",
        "        # but for pytorch\n",
        "        # [-20, 20] -> [0, 255]\n",
        "        flow_tensor = 128 + 255 / 40 * flow_tensor\n",
        "        return flow_tensor.round()\n",
        "\n",
        "\n",
        "class ToCFHW_ToFloat(object):\n",
        "\n",
        "    def __call__(self, tensor_fhwc: torch.Tensor) -> torch.Tensor:\n",
        "        return tensor_fhwc.permute(3, 0, 1, 2).float()\n",
        "\n",
        "\n",
        "class ToFCHW(object):\n",
        "\n",
        "    def __call__(self, tensor_cfhw: torch.Tensor) -> torch.Tensor:\n",
        "        return tensor_cfhw.permute(1, 0, 2, 3)\n",
        "\n",
        "\n",
        "def resize(img, size, resize_to_smaller_edge=True, interpolation=Image.BILINEAR):\n",
        "    r\"\"\"\n",
        "    (v-iashin): this is almost the same implementation as in PyTorch except it has no _is_pil_image() check\n",
        "    and has an extra argument governing what happens if `size` is `int`.\n",
        "    Reference: https://pytorch.org/docs/1.6.0/_modules/torchvision/transforms/functional.html#resize\n",
        "    Resize the input PIL Image to the given size.\n",
        "    Args:\n",
        "        img (PIL Image): Image to be resized.\n",
        "        size (sequence or int): Desired output size. If size is a sequence like\n",
        "            (h, w), the output size will be matched to this. If size is an int,\n",
        "            the smaller (bigger depending on `resize_to_smaller_edge`) edge of the image will be matched\n",
        "            to this number maintaining\n",
        "            the aspect ratio. i.e, if height > width, then image will be rescaled to\n",
        "            :math:`\\left(\\text{size} \\times \\frac{\\text{height}}{\\text{width}}, \\text{size}\\right)`\n",
        "        resize_to_smaller_edge (bool, optional): if True the smaller edge is matched to number in `size`,\n",
        "            if False, the bigger edge is matched to it.\n",
        "        interpolation (int, optional): Desired interpolation. Default is\n",
        "            ``PIL.Image.BILINEAR``\n",
        "    Returns:\n",
        "        PIL Image: Resized image.\n",
        "    \"\"\"\n",
        "    if not (isinstance(size, int) or (isinstance(size, Iterable) and len(size) == 2)):\n",
        "        raise TypeError('Got inappropriate size arg: {}'.format(size))\n",
        "\n",
        "    if isinstance(size, int):\n",
        "        w, h = img.size\n",
        "        if (w <= h and w == size) or (h <= w and h == size):\n",
        "            return img\n",
        "        if (w < h) == resize_to_smaller_edge:\n",
        "            ow = size\n",
        "            oh = int(size * h / w)\n",
        "            return img.resize((ow, oh), interpolation)\n",
        "        else:\n",
        "            oh = size\n",
        "            ow = int(size * w / h)\n",
        "            return img.resize((ow, oh), interpolation)\n",
        "    else:\n",
        "        return img.resize(size[::-1], interpolation)\n",
        "\n",
        "\n",
        "class ResizeImproved(object):\n",
        "\n",
        "    def __init__(self, size: int, resize_to_smaller_edge: bool = True, interpolation=Image.BILINEAR):\n",
        "        self.size = size\n",
        "        self.resize_to_smaller_edge = resize_to_smaller_edge\n",
        "        self.interpolation = interpolation\n",
        "\n",
        "    def __call__(self, img):\n",
        "        return resize(img, self.size, self.resize_to_smaller_edge, self.interpolation)\n",
        "\n",
        "\n",
        "class ToTensorWithoutScaling(object):\n",
        "\n",
        "    def __call__(self, np_img):\n",
        "        return torch.from_numpy(np_img).permute(2, 0, 1).float()\n",
        "\n",
        "\n",
        "class ToFloat(object):\n",
        "\n",
        "    def __call__(self, byte_img):\n",
        "        return byte_img.float()\n",
        "\n",
        "\n",
        "class PILToTensor:\n",
        "    \"\"\"Convert a ``PIL Image`` to a tensor of the same type. This transform does not support torchscript.\n",
        "    Converts a PIL Image (H x W x C) to a Tensor of shape (C x H x W).\n",
        "    Reference: https://github.com/pytorch/vision/blob/610c9d2a06/torchvision/transforms/functional.py#L107\n",
        "    \"\"\"\n",
        "\n",
        "    def __call__(self, pic):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pic (PIL Image): Image to be converted to tensor.\n",
        "        Returns:\n",
        "            Tensor: Converted image.\n",
        "        \"\"\"\n",
        "        # handle PIL Image\n",
        "        img = torch.from_numpy(np.array(pic, copy=True))\n",
        "        img = img.view(pic.size[1], pic.size[0], len(pic.getbands()))\n",
        "        # put it from HWC to CHW format\n",
        "        img = img.permute((2, 0, 1))\n",
        "        return img\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '()'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PTAaSOfKTeEs"
      },
      "outputs": [],
      "source": [
        "from torchvision.io.video import read_video\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch, gc\n",
        "import torch.nn as nn\n",
        "from typing import Tuple\n",
        "from typing import Callable\n",
        "from pytorchvideo.layers.accelerator.mobile_cpu.attention import SqueezeExcitation\n",
        "\n",
        "def round_width(width, multiplier, min_width = 8, divisor = 8, ceil = False):\n",
        "  if not multiplier:\n",
        "    return width\n",
        "  \n",
        "  width *= multiplier\n",
        "  min_width = min_width or divisor\n",
        "  if ceil:\n",
        "      width_out = max(min_width, int(math.ceil(width / divisor)) * divisor)\n",
        "  else:\n",
        "      width_out = max(min_width, int(width + divisor / 2) // divisor * divisor)\n",
        "  if width_out < 0.9 * width:\n",
        "      width_out += divisor\n",
        "  return int(width_out)\n",
        "\n",
        "def create_x3d_stem(\n",
        "    # Conv configs.\n",
        "    in_channels: int,\n",
        "    out_channels: int,\n",
        "    conv_kernel_size: Tuple[int] = (5, 3, 3),\n",
        "    conv_stride: Tuple[int] = (1, 2, 2),\n",
        "    conv_padding: Tuple[int] = (2, 1, 1),\n",
        "    # BN configs.\n",
        "    norm: Callable = nn.BatchNorm3d,\n",
        "    norm_eps: float = 1e-5,\n",
        "    norm_momentum: float = 0.1,\n",
        "    # Activation configs.\n",
        "    activation: Callable = nn.ReLU,\n",
        ") -> nn.Module:\n",
        "\n",
        "  conv_xy_module = nn.Conv3d(\n",
        "      in_channels = in_channels,\n",
        "      out_channels = out_channels,\n",
        "      kernel_size = (1, conv_kernel_size[1], conv_kernel_size[2]),\n",
        "      stride=(1, conv_stride[1], conv_stride[2]),\n",
        "      padding=(0, conv_padding[1], conv_padding[2]),\n",
        "      bias=False,\n",
        "  )\n",
        "\n",
        "  conv_t_module = nn.Conv3d(\n",
        "      in_channels = out_channels,\n",
        "      out_channels = out_channels,\n",
        "      kernel_size=(conv_kernel_size[0], 1, 1),\n",
        "      stride=(conv_stride[0], 1, 1),\n",
        "      padding=(conv_padding[0], 0, 0),\n",
        "      bias=False,\n",
        "      groups=out_channels,\n",
        "  )\n",
        "\n",
        "  stacked_conv_module = Conv2plus1d(\n",
        "      conv_t=conv_xy_module,\n",
        "      norm=None,\n",
        "      activation=None,\n",
        "      conv_xy=conv_t_module,\n",
        "  )\n",
        "\n",
        "  norm_module = (\n",
        "      None\n",
        "      if norm is None\n",
        "      else norm(num_features=out_channels, eps=norm_eps, momentum=norm_momentum)\n",
        "  )\n",
        "\n",
        "  activation_module = None if activation is None else activation()\n",
        "\n",
        "  return ResNetBasicStem(\n",
        "      conv = stacked_conv_module,\n",
        "      norm = norm_module,\n",
        "      activation = activation_module,\n",
        "      pool = None\n",
        "  )\n",
        "\n",
        "\n",
        "class Conv2plus1d(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      conv_t: nn.Module = None,\n",
        "      norm: nn.Module = None,\n",
        "      activation: nn.Module = None,\n",
        "      conv_xy: nn.Module = None,\n",
        "      conv_xy_first: bool = False,\n",
        "  ) -> None:\n",
        "    super(Conv2plus1d, self).__init__()\n",
        "    self.conv_t = conv_t\n",
        "    self.norm = norm\n",
        "    self.activation = activation\n",
        "    self.conv_xy = conv_xy\n",
        "    self.conv_xy_first = conv_xy_first\n",
        "  \n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    x = self.conv_xy(x) if self.conv_xy_first else self.conv_t(x)\n",
        "    x = self.norm(x) if self.norm else x\n",
        "    x = self.activation(x) if self.activation else x\n",
        "    x = self.conv_t(x) if self.conv_xy_first else self.conv_xy(x)\n",
        "    return x\n",
        "\n",
        "class ResNetBasicStem(nn.Module):\n",
        "  def __init__(self, \n",
        "               conv: nn.Module = None,\n",
        "               norm: nn.Module = None,\n",
        "               activation: nn.Module = None,\n",
        "               pool: nn.Module = None\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.conv = conv\n",
        "    self.norm = norm\n",
        "    self.activation = activation\n",
        "    self.pool = pool\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    if self.norm is not None:\n",
        "      x = self.norm(x)\n",
        "    if self.activation is not None:\n",
        "      x = self.activation(x)\n",
        "    if self.pool is not None:\n",
        "      x = self.pool(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    \"\"\"\n",
        "    Wrapper for the Swish activation function.\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        return SwishFunction.apply(x)\n",
        "\n",
        "\n",
        "class SwishFunction(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Implementation of the Swish activation function: x * sigmoid(x).\n",
        "    Searching for activation functions. Ramachandran, Prajit and Zoph, Barret\n",
        "    and Le, Quoc V. 2017\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x):\n",
        "        result = x * torch.sigmoid(x)\n",
        "        ctx.save_for_backward(x)\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        x = ctx.saved_variables[0]\n",
        "        sigmoid_x = torch.sigmoid(x)\n",
        "        return grad_output * (sigmoid_x * (1 + x * (1 - sigmoid_x)))\n",
        "\n",
        "\n",
        "def create_x3d_bottleneck_block(\n",
        "    dim_in: int,\n",
        "    dim_inner: int,\n",
        "    dim_out: int,\n",
        "    conv_kernel_size: Tuple[int] = (3, 3, 3),\n",
        "    conv_stride: Tuple[int] = (1, 2, 2),\n",
        "    # Norm configs.\n",
        "    norm: Callable = nn.BatchNorm3d,\n",
        "    norm_eps: float = 1e-5,\n",
        "    norm_momentum: float = 0.1,\n",
        "    se_ratio: float = 0.0625,\n",
        "    # Activation configs.\n",
        "    activation: Callable = nn.ReLU,\n",
        "    inner_act: Callable = Swish,\n",
        ") -> nn.Module:\n",
        "  \"\"\"\n",
        "  Bottleneck block for X3D: a sequence of Conv, Normalization with optional SE block,\n",
        "  and Activations\n",
        "  \"\"\"\n",
        "  conv_a = nn.Conv3d(\n",
        "      in_channels = dim_in,\n",
        "      out_channels = dim_inner,\n",
        "      kernel_size = (1, 1, 1),\n",
        "      bias = False\n",
        "  )\n",
        "  norm_a = (\n",
        "      None \n",
        "      if norm is None \n",
        "      else norm(num_features = dim_inner, eps = norm_eps, momentum = norm_momentum)\n",
        "  )\n",
        "  act_a = None if activation is None else activation()\n",
        "\n",
        "  # 3x3x3 Conv (Separable Convolution)\n",
        "  conv_b = nn.Conv3d(\n",
        "      in_channels = dim_inner,\n",
        "      out_channels = dim_inner,\n",
        "      kernel_size = conv_kernel_size,\n",
        "      stride = conv_stride,\n",
        "      padding = [size // 2 for size in conv_kernel_size],\n",
        "      bias = False,\n",
        "      groups = dim_inner,\n",
        "      dilation = (1, 1, 1)\n",
        "  )\n",
        "  se = (\n",
        "      SqueezeExcitation(\n",
        "          num_channels = dim_inner,\n",
        "          num_channels_reduced = round_width(dim_inner, se_ratio),\n",
        "          is_3d = True\n",
        "      )\n",
        "      if se_ratio > 0.0\n",
        "      else nn.Identity()\n",
        "  )\n",
        "  norm_b = nn.Sequential(\n",
        "      (\n",
        "          nn.Identity()\n",
        "          if norm is None\n",
        "          else norm(num_features = dim_inner, eps = norm_eps, momentum = norm_momentum)    \n",
        "      ),\n",
        "      se\n",
        "  )\n",
        "  act_b = None if inner_act is None else inner_act()\n",
        "\n",
        "  # 1x1x1 Conv (Separable Convolution)\n",
        "  conv_c = nn.Conv3d(\n",
        "      in_channels = dim_inner,\n",
        "      out_channels = dim_out,\n",
        "      kernel_size = (1, 1, 1),\n",
        "      bias = False\n",
        "  )\n",
        "  norm_c = (\n",
        "      None\n",
        "      if norm is None\n",
        "      else norm(num_features = dim_out, eps = norm_eps, momentum = norm_momentum)\n",
        "  )\n",
        "\n",
        "  return BottleneckBlock(\n",
        "      conv_a=conv_a,\n",
        "      norm_a=norm_a,\n",
        "      act_a=act_a,\n",
        "      conv_b=conv_b,\n",
        "      norm_b=norm_b,\n",
        "      act_b=act_b,\n",
        "      conv_c=conv_c,\n",
        "      norm_c=norm_c\n",
        "  )\n",
        "\n",
        "class BottleneckBlock(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      conv_a: nn.Module = None,\n",
        "      norm_a: nn.Module = None,\n",
        "      act_a: nn.Module = None,\n",
        "      conv_b: nn.Module = None,\n",
        "      norm_b: nn.Module = None,\n",
        "      act_b: nn.Module = None,\n",
        "      conv_c: nn.Module = None,\n",
        "      norm_c: nn.Module = None,\n",
        "  ):\n",
        "    super(BottleneckBlock, self).__init__()\n",
        "    self.conv_a = conv_a\n",
        "    self.norm_a = norm_a\n",
        "    self.act_a = act_a\n",
        "\n",
        "    self.conv_b = conv_b\n",
        "    self.norm_b = norm_b\n",
        "    self.act_b = act_b\n",
        "\n",
        "    self.conv_c = conv_c\n",
        "    self.norm_c = norm_c\n",
        "  \n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    x = self.conv_a(x)\n",
        "    x = self.norm_a(x) if self.norm_a is not None else x\n",
        "    x = self.act_a(x) if self.act_a is not None else x\n",
        "\n",
        "    x = self.conv_b(x)\n",
        "    x = self.norm_b(x) if self.norm_b is not None else x\n",
        "    x = self.act_b(x) if self.act_b is not None else x\n",
        "\n",
        "    x = self.conv_c(x)\n",
        "    x = self.norm_c(x) if self.norm_c is not None else x\n",
        "\n",
        "    return x\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      branch1_conv: nn.Module = None,\n",
        "      branch1_norm: nn.Module = None,\n",
        "      branch2: nn.Module = None,\n",
        "      activation: nn.Module = None,\n",
        "      branch_fusion: Callable = None\n",
        "  ) -> nn.Module:\n",
        "    super(ResBlock, self).__init__()\n",
        "    self.branch1_conv = branch1_conv\n",
        "    self.branch1_norm = branch1_norm\n",
        "    self.branch2 = branch2\n",
        "    self.activation = activation\n",
        "    self.branch_fusion = branch_fusion\n",
        "  \n",
        "  def forward(self, x) -> torch.Tensor:\n",
        "    if self.branch1_conv is None:\n",
        "      x = self.branch_fusion(x, self.branch2(x))\n",
        "    else:\n",
        "      shortcut = self.branch1_conv(x)\n",
        "      if self.branch1_norm is not None:\n",
        "        shortcut = self.branch1_norm(shortcut)\n",
        "      x = self.branch_fusion(shortcut, self.branch2(x))\n",
        "    \n",
        "    if self.activation is not None:\n",
        "      x = self.activation(x)\n",
        "    return x\n",
        "\n",
        "def create_x3d_res_block(\n",
        "    # Bottleneck Block configs.\n",
        "    dim_in: int,\n",
        "    dim_inner: int,\n",
        "    dim_out: int,\n",
        "    bottleneck: Callable = create_x3d_bottleneck_block,\n",
        "    use_shortcut: bool = True,\n",
        "    # Conv configs\n",
        "    conv_kernel_size: Tuple[int] = (3, 3, 3),\n",
        "    conv_stride: Tuple[int] = (1, 2, 2),\n",
        "    # Norm configs.\n",
        "    norm: Callable = nn.BatchNorm3d,\n",
        "    norm_eps: float = 1e-5,\n",
        "    norm_momentum: float = 0.1,\n",
        "    se_ratio: float = 0.0625,\n",
        "    # Activation configs.\n",
        "    activation: Callable = nn.ReLU,\n",
        "    inner_act: Callable = Swish\n",
        ") -> nn.Module:\n",
        "\n",
        "  norm_model = None\n",
        "  if norm is not None and dim_in != dim_out:\n",
        "    norm_model = norm(num_features = dim_out)\n",
        "  \n",
        "  return ResBlock(\n",
        "      branch1_conv = nn.Conv3d(dim_in, dim_out, kernel_size = (1, 1, 1), stride = conv_stride, bias = False)\n",
        "      if (dim_in != dim_out or np.prod(conv_stride) > 1) and use_shortcut\n",
        "      else None,\n",
        "      branch1_norm = norm_model if dim_in != dim_out and use_shortcut else None,\n",
        "      branch2 = bottleneck(\n",
        "          dim_in = dim_in,\n",
        "          dim_inner = dim_inner,\n",
        "          dim_out = dim_out,\n",
        "          conv_kernel_size=conv_kernel_size,\n",
        "          conv_stride=conv_stride,\n",
        "          norm=norm,\n",
        "          norm_eps=norm_eps,\n",
        "          norm_momentum=norm_momentum,\n",
        "          se_ratio=se_ratio,\n",
        "          activation=activation,\n",
        "          inner_act=inner_act\n",
        "      ),\n",
        "      activation = None if activation is None else activation(),\n",
        "      branch_fusion = lambda x, y: x + y\n",
        "  )\n",
        "\n",
        "class ResStage(nn.Module):\n",
        "  def __init__(self, res_blocks: nn.ModuleList) -> nn.Module:\n",
        "    super(ResStage, self).__init__()\n",
        "    self.res_blocks = res_blocks\n",
        "  \n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    for _, res_block in enumerate(self.res_blocks):\n",
        "      x = res_block(x)\n",
        "      \n",
        "    return x\n",
        "\n",
        "def round_repeats(repeats, multiplier):\n",
        "  if not multiplier:\n",
        "    return repeats\n",
        "  return int(math.ceil(repeats * multiplier))\n",
        "\n",
        "def create_x3d_res_stage(\n",
        "    # Stage configs\n",
        "    depth: int,\n",
        "    # Bottle Block Configs\n",
        "    dim_in: int,\n",
        "    dim_inner: int,\n",
        "    dim_out: int,\n",
        "    bottleneck: Callable = create_x3d_bottleneck_block,\n",
        "    # Conv Configs\n",
        "    conv_kernel_size: Tuple[int] = (3, 3, 3),\n",
        "    conv_stride: Tuple[int] = (1, 2, 2),\n",
        "    # Norm Configs\n",
        "    norm: Callable = nn.BatchNorm3d,\n",
        "    norm_eps: float = 1e-5,\n",
        "    norm_momentum: float = 0.1,\n",
        "    se_ratio: float = 0.0625,\n",
        "    # Activation configs.\n",
        "    activation: Callable = nn.ReLU,\n",
        "    inner_act: Callable = Swish,\n",
        ") -> nn.Module:\n",
        "  \n",
        "  res_blocks = []\n",
        "  for idx in range(depth):\n",
        "    block = create_x3d_res_block(\n",
        "        dim_in = dim_in if idx == 0 else dim_out,\n",
        "        dim_inner = dim_inner,\n",
        "        dim_out = dim_out,\n",
        "        bottleneck = bottleneck,\n",
        "        conv_kernel_size=conv_kernel_size,\n",
        "        conv_stride=conv_stride if idx == 0 else (1, 1, 1),\n",
        "        norm = norm,\n",
        "        norm_eps = norm_eps,\n",
        "        norm_momentum = norm_momentum,\n",
        "        se_ratio=(se_ratio if (idx + 1) % 2 else 0.0),\n",
        "        activation=activation,\n",
        "        inner_act=inner_act,\n",
        "    )\n",
        "\n",
        "    res_blocks.append(block)\n",
        "  \n",
        "  return ResStage(res_blocks=nn.ModuleList(res_blocks))\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self, blocks: nn.ModuleList):\n",
        "    super(Net, self).__init__()\n",
        "    self.blocks = blocks\n",
        "  \n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    for idx in range(len(self.blocks)):\n",
        "      x = self.blocks[idx](x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def create_x3d(\n",
        "    input_channel: int = 3,\n",
        "    input_clip_length: int = 13,\n",
        "    input_crop_size: int = 160,\n",
        "    # Model Configs\n",
        "    model_num_class: int = 400,\n",
        "    dropout_rate: float = 0.5,\n",
        "    width_factor: float = 2.0,\n",
        "    depth_factor: float = 2.2,\n",
        "    # Normalization configs.\n",
        "    norm: Callable = nn.BatchNorm3d,\n",
        "    norm_eps: float = 0.1,\n",
        "    norm_momentum: float = 0.1,\n",
        "    # Activation configs.\n",
        "    activation: Callable = nn.ReLU,\n",
        "    # Stem Configs\n",
        "    stem_dim_in: int = 12,\n",
        "    stem_conv_kernel_size: Tuple[int] = (5, 3, 3),\n",
        "    stem_conv_stride: Tuple[int] = (1, 2, 2),\n",
        "    # Stage configs.\n",
        "    stage_conv_kernel_size: Tuple[Tuple[int]] = (\n",
        "        (3, 3, 3),\n",
        "        (3, 3, 3),\n",
        "        (3, 3, 3),\n",
        "        (3, 3, 3),\n",
        "    ),\n",
        "    stage_spatial_stride: Tuple[int] = (2, 2, 2, 2),\n",
        "    stage_temporal_stride: Tuple[int] = (1, 1, 1, 1),\n",
        "    bottleneck: Callable = create_x3d_bottleneck_block,\n",
        "    bottleneck_factor: float = 2.25,\n",
        "    se_ratio: float = 0.0625,\n",
        "    inner_act: Callable = Swish,\n",
        "    # Head configs.\n",
        "    head_dim_out: int = 2048,\n",
        "    head_pool_act: Callable = nn.ReLU,\n",
        "    head_bn_lin5_on: bool = False,\n",
        "    head_activation: Callable = nn.Softmax,\n",
        "    head_output_with_global_average: bool = True,\n",
        ") -> nn.Module:\n",
        "\n",
        "  # stem_dim_in = 12\n",
        "  blocks = []\n",
        "  stem_dim_out = round_width(stem_dim_in, width_factor) # 24\n",
        "  stem = create_x3d_stem(\n",
        "      in_channels = input_channel,\n",
        "      out_channels = stem_dim_out,\n",
        "      conv_kernel_size = stem_conv_kernel_size,\n",
        "      conv_stride = stem_conv_stride,\n",
        "      conv_padding=[size // 2 for size in stem_conv_kernel_size],\n",
        "      norm=norm,\n",
        "      norm_eps=norm_eps,\n",
        "      norm_momentum=norm_momentum,\n",
        "      activation=activation,\n",
        "  )\n",
        "\n",
        "  # return stem\n",
        "\n",
        "  blocks.append(stem)\n",
        "\n",
        "  # Compute the depth and dimension for each stage\n",
        "  stage_depths = [1, 2, 5, 3]\n",
        "  exp_stage = 2.0\n",
        "  stage_dim1 = stem_dim_in # 12\n",
        "  stage_dim2 = round_width(stage_dim1, exp_stage, divisor = 8) # 24\n",
        "  stage_dim3 = round_width(stage_dim2, exp_stage, divisor = 8) # 48\n",
        "  stage_dim4 = round_width(stage_dim3, exp_stage, divisor=8) # 96\n",
        "  stage_dims = [stage_dim1, stage_dim2, stage_dim3, stage_dim4] # 12, 24, 48, 96\n",
        "\n",
        "  # print(stage_dim1, stage_dim2, stage_dim3, stage_dim4)\n",
        "\n",
        "  dim_in = stem_dim_out\n",
        "\n",
        "  for idx in range(len(stage_dims)):\n",
        "    dim_out = round_width(stage_dims[idx], width_factor) # 24, 48, 96, 192\n",
        "    # print(dim_out)\n",
        "    dim_inner = int(bottleneck_factor * dim_out) # 54, 108, 216, 432\n",
        "    # print(dim_inner)\n",
        "    depth = round_repeats(stage_depths[idx], depth_factor) # 3, 5, 11, 7\n",
        "    # print(depth)\n",
        "\n",
        "    stage_conv_stride = (\n",
        "        stage_temporal_stride[idx],\n",
        "        stage_spatial_stride[idx],\n",
        "        stage_spatial_stride[idx],\n",
        "    ) # (1, 2, 2), (1, 2, 2), (1, 2, 2), (1, 2, 2)\n",
        "    # print(stage_conv_stride)\n",
        "\n",
        "    stage = create_x3d_res_stage(\n",
        "        depth=depth,\n",
        "        dim_in=dim_in,\n",
        "        dim_inner=dim_inner,\n",
        "        dim_out=dim_out,\n",
        "        bottleneck=bottleneck,\n",
        "        conv_kernel_size=stage_conv_kernel_size[idx],\n",
        "        conv_stride=stage_conv_stride,\n",
        "        norm=norm,\n",
        "        norm_eps=norm_eps,\n",
        "        norm_momentum=norm_momentum,\n",
        "        se_ratio=se_ratio,\n",
        "        activation=activation,\n",
        "        inner_act=inner_act,\n",
        "    )\n",
        "\n",
        "    blocks.append(stage)\n",
        "    dim_in = dim_out\n",
        "  \n",
        "  # return nn.ModuleList(blocks)\n",
        "\n",
        "  # Create head for X3D.\n",
        "  total_spatial_stride = stem_conv_stride[1] * np.prod(stage_spatial_stride) # 32\n",
        "  total_temporal_stride = stem_conv_stride[0] * np.prod(stage_temporal_stride) # 1\n",
        "  \n",
        "  assert (\n",
        "      input_clip_length >= total_temporal_stride\n",
        "  ), \"Clip length doesn't match temporal stride!\"\n",
        "  \n",
        "  assert (\n",
        "      input_crop_size >= total_spatial_stride\n",
        "  ), \"Crop size doesn't match spatial stride!\"\n",
        "\n",
        "  head_pool_kernel_size = (\n",
        "      input_clip_length // total_temporal_stride,\n",
        "      int(math.ceil(input_crop_size / total_spatial_stride)),\n",
        "      int(math.ceil(input_crop_size / total_spatial_stride))\n",
        "  ) # (13, 5, 5)\n",
        "\n",
        "  head = create_x3d_head(\n",
        "      dim_in = dim_out,\n",
        "      dim_inner = dim_inner,\n",
        "      dim_out = head_dim_out,\n",
        "      num_classes = model_num_class,\n",
        "      pool_act = head_pool_act,\n",
        "      pool_kernel_size = head_pool_kernel_size,\n",
        "      norm = norm,\n",
        "      norm_eps = norm_eps,\n",
        "      norm_momentum = norm_momentum,\n",
        "      bn_lin5_on = head_bn_lin5_on,\n",
        "      dropout_rate = dropout_rate,\n",
        "      activation = head_activation,\n",
        "      output_with_global_average = head_output_with_global_average\n",
        "  )\n",
        "\n",
        "  # blocks.append(head)\n",
        "  # block_head = []\n",
        "  # block_head.append(head)\n",
        "\n",
        "  # return nn.ModuleList(block_head)\n",
        "\n",
        "  blocks.append(head)\n",
        "  # return nn.ModuleList(blocks)\n",
        "  return Net(blocks = nn.ModuleList(blocks))\n",
        "\n",
        "def create_x3d_head(\n",
        "    dim_in: int,\n",
        "    dim_inner: int,\n",
        "    dim_out: int,\n",
        "    num_classes: int,\n",
        "    # Pooling Configs\n",
        "    pool_act: Callable = nn.ReLU,\n",
        "    pool_kernel_size: Tuple[int] = (13, 5, 5),\n",
        "    # BN Configs\n",
        "    norm: Callable = nn.BatchNorm3d,\n",
        "    norm_eps: float = 1e-5,\n",
        "    norm_momentum: float = 0.1,\n",
        "    bn_lin5_on = False,\n",
        "    # Dropout configs.\n",
        "    dropout_rate: float = 0.5,\n",
        "    # Activation configs.\n",
        "    activation: Callable = nn.Softmax,\n",
        "    # Output configs.\n",
        "    output_with_global_average: bool = True,\n",
        ") -> nn.Module:\n",
        "\n",
        "  pre_conv_module = nn.Conv3d(\n",
        "      in_channels = dim_in, out_channels = dim_inner, kernel_size = (1, 1, 1), bias = False\n",
        "  )\n",
        "  pre_norm_module = norm(num_features = dim_inner, eps = norm_eps, momentum = norm_momentum)\n",
        "  pre_act_module = None if pool_act is None else pool_act()\n",
        "\n",
        "\n",
        "  if pool_kernel_size is None:\n",
        "    pool_module = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
        "  else:\n",
        "    pool_module = nn.AvgPool3d(pool_kernel_size, stride = 1)\n",
        "\n",
        "\n",
        "  post_conv_module = nn.Conv3d(\n",
        "      in_channels = dim_inner, out_channels=dim_out, kernel_size=(1, 1, 1), bias=False\n",
        "  ) # ***************************(2048)***************************\n",
        "  if bn_lin5_on:\n",
        "    post_norm_module = norm(\n",
        "      num_features = dim_out, eps = norm_eps, momentum = norm_momentum\n",
        "    )\n",
        "  else:\n",
        "    post_norm_module = None\n",
        "  # post_act_module = None if pool_act is None else pool_act() # Sửa ở đây\n",
        "  post_act_module = None\n",
        "\n",
        "  projected_pool_module = ProjectedPool(\n",
        "    pre_conv = pre_conv_module,\n",
        "    pre_norm = pre_norm_module,\n",
        "    pre_act = pre_act_module,\n",
        "    pool = pool_module,\n",
        "    post_conv = post_conv_module,\n",
        "    post_norm = post_norm_module,\n",
        "    post_act = post_act_module,\n",
        "  )\n",
        "\n",
        "  if activation is None:\n",
        "    activation_module = None\n",
        "  elif activation == nn.Softmax:\n",
        "    activation_module = activation(dim=1)\n",
        "  elif activation == nn.Sigmoid:\n",
        "    activation_module = activation()\n",
        "  else:\n",
        "    raise NotImplementedError(\n",
        "        \"{} is not supported as an activation\" \"function.\".format(activation)\n",
        "    )\n",
        "\n",
        "  if output_with_global_average:\n",
        "    output_pool = nn.AdaptiveAvgPool3d(1)\n",
        "  else:\n",
        "    output_pool = None\n",
        "  \n",
        "  # return ResNetBasicHead(\n",
        "  #     proj = nn.Linear(dim_out, num_classes, bias=True),\n",
        "  #     activation = activation_module,\n",
        "  #     pool = projected_pool_module,\n",
        "  #     dropout = nn.Dropout(dropout_rate) if dropout_rate > 0 else None,\n",
        "  #     output_pool = output_pool,\n",
        "  # ) # Sửa ở đây\n",
        "  \n",
        "  return ResNetBasicHead(\n",
        "      pool = projected_pool_module\n",
        "  )\n",
        "\n",
        "class ProjectedPool(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      pre_conv: nn.Module = None,\n",
        "      pre_norm: nn.Module = None,\n",
        "      pre_act: nn.Module = None,\n",
        "      pool: nn.Module = None,\n",
        "      post_conv: nn.Module = None,\n",
        "      post_norm: nn.Module = None,\n",
        "      post_act: nn.Module = None,\n",
        "  ):\n",
        "\n",
        "    super(ProjectedPool, self).__init__()\n",
        "    self.pre_conv = pre_conv\n",
        "    self.pre_norm = pre_norm\n",
        "    self.pre_act = pre_act\n",
        "\n",
        "    self.pool = pool\n",
        "\n",
        "    self.post_conv = post_conv\n",
        "    self.post_norm = post_norm\n",
        "    self.post_act = post_act\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pre_conv(x)\n",
        "    if self.pre_norm is not None:\n",
        "      x = self.pre_norm(x)\n",
        "    if self.pre_act is not None:\n",
        "      x = self.pre_act(x)\n",
        "    \n",
        "    x = self.pool(x)\n",
        "    \n",
        "    x = self.post_conv(x)\n",
        "    if self.post_norm is not None:\n",
        "      x = self.post_norm(x)\n",
        "    if self.post_act is not None:\n",
        "      x = self.post_act(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "class ResNetBasicHead(nn.Module):\n",
        "  def __init__(\n",
        "    self,\n",
        "    pool: nn.Module = None,\n",
        "    dropout: nn.Module = None,\n",
        "    proj: nn.Module = None,\n",
        "    activation: nn.Module = None,\n",
        "    output_pool: nn.Module = None,\n",
        "  ):\n",
        "\n",
        "    super(ResNetBasicHead, self).__init__()\n",
        "    self.pool = pool\n",
        "    self.dropout = dropout\n",
        "    self.proj = proj\n",
        "    self.activation = activation\n",
        "    self.output_pool = output_pool\n",
        "  \n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    if self.pool is not None:\n",
        "      x = self.pool(x)\n",
        "    \n",
        "    if self.dropout is not None:\n",
        "      x = self.dropout(x)\n",
        "\n",
        "    if self.proj is not None:\n",
        "      x = x.permute((0, 2, 3, 4, 1))\n",
        "      x = self.proj(x)\n",
        "      x = x.permute((0, 4, 1, 2, 3))\n",
        "    \n",
        "    if self.activation is not None:\n",
        "      x = self.activation(x)\n",
        "\n",
        "    if self.output_pool is not None:\n",
        "      # Performs global averaging.\n",
        "      x = self.output_pool(x)\n",
        "      x = x.view(x.shape[0], -1)\n",
        "\n",
        "    return x\n",
        "\n",
        "\"\"\"\n",
        "import json\n",
        "import urllib\n",
        "from pytorchvideo.data.encoded_video import EncodedVideo\n",
        "\n",
        "from torchvision.transforms import Compose, Lambda\n",
        "from torchvision.transforms._transforms_video import (\n",
        "    CenterCropVideo,\n",
        "    NormalizeVideo,\n",
        ")\n",
        "from pytorchvideo.transforms import (\n",
        "    ApplyTransformToKey,\n",
        "    ShortSideScale,\n",
        "    UniformTemporalSubsample\n",
        ")\n",
        "\n",
        "mean = [0.45, 0.45, 0.45]\n",
        "std = [0.225, 0.225, 0.225]\n",
        "frames_per_second = 30\n",
        "model_transform_params  = {\n",
        "        \"side_size\": 256,\n",
        "        \"crop_size\": 256,\n",
        "        \"num_frames\": 16,\n",
        "        \"sampling_rate\": 5,\n",
        "}\n",
        "\n",
        "# Get transform parameters based on model\n",
        "transform_params = model_transform_params\n",
        "\n",
        "transform =  ApplyTransformToKey(\n",
        "    key=\"video\",\n",
        "    transform=Compose(\n",
        "        [\n",
        "            UniformTemporalSubsample(transform_params[\"num_frames\"]),\n",
        "            Lambda(lambda x: x/255.0),\n",
        "            NormalizeVideo(mean, std),\n",
        "            ShortSideScale(size=transform_params[\"side_size\"]),\n",
        "            CenterCropVideo(\n",
        "                crop_size=(transform_params[\"crop_size\"], transform_params[\"crop_size\"])\n",
        "            )\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "import csv\n",
        "from pytorchvideo.data.encoded_video import EncodedVideo\n",
        "from torchvision.transforms import Compose, Lambda\n",
        "from torchvision.transforms._transforms_video import (\n",
        "    CenterCropVideo,\n",
        "    NormalizeVideo,\n",
        ")\n",
        "from pytorchvideo.transforms import (\n",
        "    ApplyTransformToKey,\n",
        "    ShortSideScale,\n",
        "    UniformTemporalSubsample\n",
        ")\n",
        "\n",
        "mean = [0.45, 0.45, 0.45]\n",
        "std = [0.225, 0.225, 0.225]\n",
        "frames_per_second = 10\n",
        "model_transform_params  = {\n",
        "    \"x3d_m\": {\n",
        "        \"side_size\": 224,\n",
        "        \"crop_size\": 224,\n",
        "        \"num_frames\": 16,\n",
        "        \"sampling_rate\": 5,\n",
        "    }\n",
        "}\n",
        "\n",
        "# Get transform parameters based on model\n",
        "transform_params = model_transform_params['x3d_m']\n",
        "\n",
        "# Note that this transform is specific to the slow_R50 model.\n",
        "transform =  ApplyTransformToKey(\n",
        "    key=\"video\",\n",
        "    transform=Compose(\n",
        "        [\n",
        "            UniformTemporalSubsample(transform_params[\"num_frames\"]),\n",
        "            Lambda(lambda x: x/255.0),\n",
        "            NormalizeVideo(mean, std),\n",
        "            ShortSideScale(size=transform_params[\"side_size\"]),\n",
        "            CenterCropVideo(\n",
        "                crop_size=(transform_params[\"crop_size\"], transform_params[\"crop_size\"])\n",
        "            )\n",
        "        ]\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpzptzBUTZ1M",
        "outputId": "b8bf7464-04ee-4a2a-92e6-151977d16dbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Load model successfully!!!\n",
            "/content/drive/MyDrive/extra_resize/b06.mp4\n",
            "[1 / 37] , 0.02702702702702703\n",
            "\n",
            "one video percent : 0 / 110\n",
            "one video percent : 1 / 110\n",
            "one video percent : 2 / 110\n",
            "one video percent : 3 / 110\n",
            "one video percent : 4 / 110\n",
            "one video percent : 5 / 110\n",
            "one video percent : 6 / 110\n",
            "one video percent : 7 / 110\n",
            "one video percent : 8 / 110\n",
            "one video percent : 9 / 110\n",
            "one video percent : 10 / 110\n",
            "one video percent : 11 / 110\n",
            "one video percent : 12 / 110\n",
            "one video percent : 13 / 110\n",
            "one video percent : 14 / 110\n",
            "one video percent : 15 / 110\n",
            "one video percent : 16 / 110\n",
            "one video percent : 17 / 110\n",
            "one video percent : 18 / 110\n",
            "one video percent : 19 / 110\n",
            "one video percent : 20 / 110\n",
            "one video percent : 21 / 110\n",
            "one video percent : 22 / 110\n",
            "one video percent : 23 / 110\n",
            "one video percent : 24 / 110\n",
            "one video percent : 25 / 110\n",
            "one video percent : 26 / 110\n",
            "one video percent : 27 / 110\n",
            "one video percent : 28 / 110\n",
            "one video percent : 29 / 110\n",
            "one video percent : 30 / 110\n",
            "one video percent : 31 / 110\n",
            "one video percent : 32 / 110\n",
            "one video percent : 33 / 110\n",
            "one video percent : 34 / 110\n",
            "one video percent : 35 / 110\n",
            "one video percent : 36 / 110\n",
            "one video percent : 37 / 110\n",
            "one video percent : 38 / 110\n",
            "one video percent : 39 / 110\n",
            "one video percent : 40 / 110\n",
            "one video percent : 41 / 110\n",
            "one video percent : 42 / 110\n",
            "one video percent : 43 / 110\n",
            "one video percent : 44 / 110\n",
            "one video percent : 45 / 110\n",
            "one video percent : 46 / 110\n",
            "one video percent : 47 / 110\n",
            "one video percent : 48 / 110\n",
            "one video percent : 49 / 110\n",
            "one video percent : 50 / 110\n",
            "one video percent : 51 / 110\n",
            "one video percent : 52 / 110\n",
            "one video percent : 53 / 110\n",
            "one video percent : 54 / 110\n",
            "one video percent : 55 / 110\n",
            "one video percent : 56 / 110\n",
            "one video percent : 57 / 110\n",
            "one video percent : 58 / 110\n",
            "one video percent : 59 / 110\n",
            "one video percent : 60 / 110\n",
            "one video percent : 61 / 110\n",
            "one video percent : 62 / 110\n",
            "one video percent : 63 / 110\n",
            "one video percent : 64 / 110\n",
            "one video percent : 65 / 110\n",
            "one video percent : 66 / 110\n",
            "one video percent : 67 / 110\n",
            "one video percent : 68 / 110\n",
            "one video percent : 69 / 110\n",
            "one video percent : 70 / 110\n",
            "one video percent : 71 / 110\n",
            "one video percent : 72 / 110\n",
            "one video percent : 73 / 110\n",
            "one video percent : 74 / 110\n",
            "one video percent : 75 / 110\n",
            "one video percent : 76 / 110\n",
            "one video percent : 77 / 110\n",
            "one video percent : 78 / 110\n",
            "one video percent : 79 / 110\n",
            "one video percent : 80 / 110\n",
            "one video percent : 81 / 110\n",
            "one video percent : 82 / 110\n",
            "one video percent : 83 / 110\n",
            "one video percent : 84 / 110\n",
            "one video percent : 85 / 110\n",
            "one video percent : 86 / 110\n",
            "one video percent : 87 / 110\n",
            "one video percent : 88 / 110\n",
            "one video percent : 89 / 110\n",
            "one video percent : 90 / 110\n",
            "one video percent : 91 / 110\n",
            "one video percent : 92 / 110\n",
            "one video percent : 93 / 110\n",
            "one video percent : 94 / 110\n",
            "one video percent : 95 / 110\n",
            "one video percent : 96 / 110\n",
            "one video percent : 97 / 110\n",
            "one video percent : 98 / 110\n",
            "one video percent : 99 / 110\n",
            "one video percent : 100 / 110\n",
            "one video percent : 101 / 110\n",
            "one video percent : 102 / 110\n",
            "one video percent : 103 / 110\n",
            "one video percent : 104 / 110\n",
            "one video percent : 105 / 110\n",
            "one video percent : 106 / 110\n",
            "one video percent : 107 / 110\n",
            "one video percent : 108 / 110\n",
            "one video percent : 109 / 110\n",
            "/content/drive/MyDrive/extra_resize/b07.mp4\n",
            "[2 / 37] , 0.05405405405405406\n",
            "\n",
            "one video percent : 0 / 51\n",
            "one video percent : 1 / 51\n",
            "one video percent : 2 / 51\n",
            "one video percent : 3 / 51\n",
            "one video percent : 4 / 51\n",
            "one video percent : 5 / 51\n",
            "one video percent : 6 / 51\n",
            "one video percent : 7 / 51\n",
            "one video percent : 8 / 51\n",
            "one video percent : 9 / 51\n",
            "one video percent : 10 / 51\n",
            "one video percent : 11 / 51\n",
            "one video percent : 12 / 51\n",
            "one video percent : 13 / 51\n",
            "one video percent : 14 / 51\n",
            "one video percent : 15 / 51\n",
            "one video percent : 16 / 51\n",
            "one video percent : 17 / 51\n",
            "one video percent : 18 / 51\n",
            "one video percent : 19 / 51\n",
            "one video percent : 20 / 51\n",
            "one video percent : 21 / 51\n",
            "one video percent : 22 / 51\n",
            "one video percent : 23 / 51\n",
            "one video percent : 24 / 51\n",
            "one video percent : 25 / 51\n",
            "one video percent : 26 / 51\n",
            "one video percent : 27 / 51\n",
            "one video percent : 28 / 51\n",
            "one video percent : 29 / 51\n",
            "one video percent : 30 / 51\n",
            "one video percent : 31 / 51\n",
            "one video percent : 32 / 51\n",
            "one video percent : 33 / 51\n",
            "one video percent : 34 / 51\n",
            "one video percent : 35 / 51\n",
            "one video percent : 36 / 51\n",
            "one video percent : 37 / 51\n",
            "one video percent : 38 / 51\n",
            "one video percent : 39 / 51\n",
            "one video percent : 40 / 51\n",
            "one video percent : 41 / 51\n",
            "one video percent : 42 / 51\n",
            "one video percent : 43 / 51\n",
            "one video percent : 44 / 51\n",
            "one video percent : 45 / 51\n",
            "one video percent : 46 / 51\n",
            "one video percent : 47 / 51\n",
            "one video percent : 48 / 51\n",
            "one video percent : 49 / 51\n",
            "one video percent : 50 / 51\n",
            "/content/drive/MyDrive/extra_resize/b08.mp4\n",
            "[3 / 37] , 0.08108108108108109\n",
            "\n",
            "one video percent : 0 / 133\n",
            "one video percent : 1 / 133\n",
            "one video percent : 2 / 133\n",
            "one video percent : 3 / 133\n",
            "one video percent : 4 / 133\n",
            "one video percent : 5 / 133\n",
            "one video percent : 6 / 133\n",
            "one video percent : 7 / 133\n",
            "one video percent : 8 / 133\n",
            "one video percent : 9 / 133\n",
            "one video percent : 10 / 133\n",
            "one video percent : 11 / 133\n",
            "one video percent : 12 / 133\n",
            "one video percent : 13 / 133\n",
            "one video percent : 14 / 133\n",
            "one video percent : 15 / 133\n",
            "one video percent : 16 / 133\n",
            "one video percent : 17 / 133\n",
            "one video percent : 18 / 133\n",
            "one video percent : 19 / 133\n",
            "one video percent : 20 / 133\n",
            "one video percent : 21 / 133\n",
            "one video percent : 22 / 133\n",
            "one video percent : 23 / 133\n",
            "one video percent : 24 / 133\n",
            "one video percent : 25 / 133\n",
            "one video percent : 26 / 133\n",
            "one video percent : 27 / 133\n",
            "one video percent : 28 / 133\n",
            "one video percent : 29 / 133\n",
            "one video percent : 30 / 133\n",
            "one video percent : 31 / 133\n",
            "one video percent : 32 / 133\n",
            "one video percent : 33 / 133\n",
            "one video percent : 34 / 133\n",
            "one video percent : 35 / 133\n",
            "one video percent : 36 / 133\n",
            "one video percent : 37 / 133\n",
            "one video percent : 38 / 133\n",
            "one video percent : 39 / 133\n",
            "one video percent : 40 / 133\n",
            "one video percent : 41 / 133\n",
            "one video percent : 42 / 133\n",
            "one video percent : 43 / 133\n",
            "one video percent : 44 / 133\n",
            "one video percent : 45 / 133\n",
            "one video percent : 46 / 133\n",
            "one video percent : 47 / 133\n",
            "one video percent : 48 / 133\n",
            "one video percent : 49 / 133\n",
            "one video percent : 50 / 133\n",
            "one video percent : 51 / 133\n",
            "one video percent : 52 / 133\n",
            "one video percent : 53 / 133\n",
            "one video percent : 54 / 133\n",
            "one video percent : 55 / 133\n",
            "one video percent : 56 / 133\n",
            "one video percent : 57 / 133\n",
            "one video percent : 58 / 133\n",
            "one video percent : 59 / 133\n",
            "one video percent : 60 / 133\n",
            "one video percent : 61 / 133\n",
            "one video percent : 62 / 133\n",
            "one video percent : 63 / 133\n",
            "one video percent : 64 / 133\n",
            "one video percent : 65 / 133\n",
            "one video percent : 66 / 133\n",
            "one video percent : 67 / 133\n",
            "one video percent : 68 / 133\n",
            "one video percent : 69 / 133\n",
            "one video percent : 70 / 133\n",
            "one video percent : 71 / 133\n",
            "one video percent : 72 / 133\n",
            "one video percent : 73 / 133\n",
            "one video percent : 74 / 133\n",
            "one video percent : 75 / 133\n",
            "one video percent : 76 / 133\n",
            "one video percent : 77 / 133\n",
            "one video percent : 78 / 133\n",
            "one video percent : 79 / 133\n",
            "one video percent : 80 / 133\n",
            "one video percent : 81 / 133\n",
            "one video percent : 82 / 133\n",
            "one video percent : 83 / 133\n",
            "one video percent : 84 / 133\n",
            "one video percent : 85 / 133\n",
            "one video percent : 86 / 133\n",
            "one video percent : 87 / 133\n",
            "one video percent : 88 / 133\n",
            "one video percent : 89 / 133\n",
            "one video percent : 90 / 133\n",
            "one video percent : 91 / 133\n",
            "one video percent : 92 / 133\n",
            "one video percent : 93 / 133\n",
            "one video percent : 94 / 133\n",
            "one video percent : 95 / 133\n",
            "one video percent : 96 / 133\n",
            "one video percent : 97 / 133\n",
            "one video percent : 98 / 133\n",
            "one video percent : 99 / 133\n",
            "one video percent : 100 / 133\n",
            "one video percent : 101 / 133\n",
            "one video percent : 102 / 133\n",
            "one video percent : 103 / 133\n",
            "one video percent : 104 / 133\n",
            "one video percent : 105 / 133\n",
            "one video percent : 106 / 133\n",
            "one video percent : 107 / 133\n",
            "one video percent : 108 / 133\n",
            "one video percent : 109 / 133\n",
            "one video percent : 110 / 133\n",
            "one video percent : 111 / 133\n",
            "one video percent : 112 / 133\n",
            "one video percent : 113 / 133\n",
            "one video percent : 114 / 133\n",
            "one video percent : 115 / 133\n",
            "one video percent : 116 / 133\n",
            "one video percent : 117 / 133\n",
            "one video percent : 118 / 133\n",
            "one video percent : 119 / 133\n",
            "one video percent : 120 / 133\n",
            "one video percent : 121 / 133\n",
            "one video percent : 122 / 133\n",
            "one video percent : 123 / 133\n",
            "one video percent : 124 / 133\n",
            "one video percent : 125 / 133\n",
            "one video percent : 126 / 133\n",
            "one video percent : 127 / 133\n",
            "one video percent : 128 / 133\n",
            "one video percent : 129 / 133\n",
            "one video percent : 130 / 133\n",
            "one video percent : 131 / 133\n",
            "one video percent : 132 / 133\n",
            "/content/drive/MyDrive/extra_resize/b09.mp4\n",
            "[4 / 37] , 0.10810810810810811\n",
            "\n",
            "one video percent : 0 / 58\n",
            "one video percent : 1 / 58\n",
            "one video percent : 2 / 58\n",
            "one video percent : 3 / 58\n",
            "one video percent : 4 / 58\n",
            "one video percent : 5 / 58\n",
            "one video percent : 6 / 58\n",
            "one video percent : 7 / 58\n",
            "one video percent : 8 / 58\n",
            "one video percent : 9 / 58\n",
            "one video percent : 10 / 58\n",
            "one video percent : 11 / 58\n",
            "one video percent : 12 / 58\n",
            "one video percent : 13 / 58\n",
            "one video percent : 14 / 58\n",
            "one video percent : 15 / 58\n",
            "one video percent : 16 / 58\n",
            "one video percent : 17 / 58\n",
            "one video percent : 18 / 58\n",
            "one video percent : 19 / 58\n",
            "one video percent : 20 / 58\n",
            "one video percent : 21 / 58\n",
            "one video percent : 22 / 58\n",
            "one video percent : 23 / 58\n",
            "one video percent : 24 / 58\n",
            "one video percent : 25 / 58\n",
            "one video percent : 26 / 58\n",
            "one video percent : 27 / 58\n",
            "one video percent : 28 / 58\n",
            "one video percent : 29 / 58\n",
            "one video percent : 30 / 58\n",
            "one video percent : 31 / 58\n",
            "one video percent : 32 / 58\n",
            "one video percent : 33 / 58\n",
            "one video percent : 34 / 58\n",
            "one video percent : 35 / 58\n",
            "one video percent : 36 / 58\n",
            "one video percent : 37 / 58\n",
            "one video percent : 38 / 58\n",
            "one video percent : 39 / 58\n",
            "one video percent : 40 / 58\n",
            "one video percent : 41 / 58\n",
            "one video percent : 42 / 58\n",
            "one video percent : 43 / 58\n",
            "one video percent : 44 / 58\n",
            "one video percent : 45 / 58\n",
            "one video percent : 46 / 58\n",
            "one video percent : 47 / 58\n",
            "one video percent : 48 / 58\n",
            "one video percent : 49 / 58\n",
            "one video percent : 50 / 58\n",
            "one video percent : 51 / 58\n",
            "one video percent : 52 / 58\n",
            "one video percent : 53 / 58\n",
            "one video percent : 54 / 58\n",
            "one video percent : 55 / 58\n",
            "one video percent : 56 / 58\n",
            "one video percent : 57 / 58\n",
            "/content/drive/MyDrive/extra_resize/b10.mp4\n",
            "[5 / 37] , 0.13513513513513514\n",
            "\n",
            "one video percent : 0 / 41\n",
            "one video percent : 1 / 41\n",
            "one video percent : 2 / 41\n",
            "one video percent : 3 / 41\n",
            "one video percent : 4 / 41\n",
            "one video percent : 5 / 41\n",
            "one video percent : 6 / 41\n",
            "one video percent : 7 / 41\n",
            "one video percent : 8 / 41\n",
            "one video percent : 9 / 41\n",
            "one video percent : 10 / 41\n",
            "one video percent : 11 / 41\n",
            "one video percent : 12 / 41\n",
            "one video percent : 13 / 41\n",
            "one video percent : 14 / 41\n",
            "one video percent : 15 / 41\n",
            "one video percent : 16 / 41\n",
            "one video percent : 17 / 41\n",
            "one video percent : 18 / 41\n",
            "one video percent : 19 / 41\n",
            "one video percent : 20 / 41\n",
            "one video percent : 21 / 41\n",
            "one video percent : 22 / 41\n",
            "one video percent : 23 / 41\n",
            "one video percent : 24 / 41\n",
            "one video percent : 25 / 41\n",
            "one video percent : 26 / 41\n",
            "one video percent : 27 / 41\n",
            "one video percent : 28 / 41\n",
            "one video percent : 29 / 41\n",
            "one video percent : 30 / 41\n",
            "one video percent : 31 / 41\n",
            "one video percent : 32 / 41\n",
            "one video percent : 33 / 41\n",
            "one video percent : 34 / 41\n",
            "one video percent : 35 / 41\n",
            "one video percent : 36 / 41\n",
            "one video percent : 37 / 41\n",
            "one video percent : 38 / 41\n",
            "one video percent : 39 / 41\n",
            "one video percent : 40 / 41\n",
            "/content/drive/MyDrive/extra_resize/h01.mp4\n",
            "[6 / 37] , 0.16216216216216217\n",
            "\n",
            "one video percent : 0 / 211\n",
            "one video percent : 1 / 211\n",
            "one video percent : 2 / 211\n",
            "one video percent : 3 / 211\n",
            "one video percent : 4 / 211\n",
            "one video percent : 5 / 211\n",
            "one video percent : 6 / 211\n",
            "one video percent : 7 / 211\n",
            "one video percent : 8 / 211\n",
            "one video percent : 9 / 211\n",
            "one video percent : 10 / 211\n",
            "one video percent : 11 / 211\n",
            "one video percent : 12 / 211\n",
            "one video percent : 13 / 211\n",
            "one video percent : 14 / 211\n",
            "one video percent : 15 / 211\n",
            "one video percent : 16 / 211\n",
            "one video percent : 17 / 211\n",
            "one video percent : 18 / 211\n",
            "one video percent : 19 / 211\n",
            "one video percent : 20 / 211\n",
            "one video percent : 21 / 211\n",
            "one video percent : 22 / 211\n",
            "one video percent : 23 / 211\n",
            "one video percent : 24 / 211\n",
            "one video percent : 25 / 211\n",
            "one video percent : 26 / 211\n",
            "one video percent : 27 / 211\n",
            "one video percent : 28 / 211\n",
            "one video percent : 29 / 211\n",
            "one video percent : 30 / 211\n",
            "one video percent : 31 / 211\n",
            "one video percent : 32 / 211\n",
            "one video percent : 33 / 211\n",
            "one video percent : 34 / 211\n",
            "one video percent : 35 / 211\n",
            "one video percent : 36 / 211\n",
            "one video percent : 37 / 211\n",
            "one video percent : 38 / 211\n",
            "one video percent : 39 / 211\n",
            "one video percent : 40 / 211\n",
            "one video percent : 41 / 211\n",
            "one video percent : 42 / 211\n",
            "one video percent : 43 / 211\n",
            "one video percent : 44 / 211\n",
            "one video percent : 45 / 211\n",
            "one video percent : 46 / 211\n",
            "one video percent : 47 / 211\n",
            "one video percent : 48 / 211\n",
            "one video percent : 49 / 211\n",
            "one video percent : 50 / 211\n",
            "one video percent : 51 / 211\n",
            "one video percent : 52 / 211\n",
            "one video percent : 53 / 211\n",
            "one video percent : 54 / 211\n",
            "one video percent : 55 / 211\n",
            "one video percent : 56 / 211\n",
            "one video percent : 57 / 211\n",
            "one video percent : 58 / 211\n",
            "one video percent : 59 / 211\n",
            "one video percent : 60 / 211\n",
            "one video percent : 61 / 211\n",
            "one video percent : 62 / 211\n",
            "one video percent : 63 / 211\n",
            "one video percent : 64 / 211\n",
            "one video percent : 65 / 211\n",
            "one video percent : 66 / 211\n",
            "one video percent : 67 / 211\n",
            "one video percent : 68 / 211\n",
            "one video percent : 69 / 211\n",
            "one video percent : 70 / 211\n",
            "one video percent : 71 / 211\n",
            "one video percent : 72 / 211\n",
            "one video percent : 73 / 211\n",
            "one video percent : 74 / 211\n",
            "one video percent : 75 / 211\n",
            "one video percent : 76 / 211\n",
            "one video percent : 77 / 211\n",
            "one video percent : 78 / 211\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from torchvision.io.video import read_video\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch, gdalconst\n",
        "import cv2\n",
        "import math\n",
        "import time\n",
        "\n",
        "def video_info(infilename):\n",
        " \n",
        "    cap = cv2.VideoCapture(infilename)\n",
        " \n",
        "    if not cap.isOpened():\n",
        "        print(\"could not open :\", infilename)\n",
        "        exit(0)\n",
        " \n",
        "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        " \n",
        "    print('length : ', length)\n",
        "    print('width : ', width)\n",
        "    print('height : ', height)\n",
        "    print('count : ', count)\n",
        "    print('fps : ', fps)\n",
        "\n",
        "def get_fps(filename):\n",
        "    cap = cv2.VideoCapture(filename)\n",
        " \n",
        "    if not cap.isOpened():\n",
        "        print(\"could not open :\", filename)\n",
        "        exit(0)\n",
        " \n",
        "    return int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "\n",
        "def get_count(filename):\n",
        "    cap = cv2.VideoCapture(filename)\n",
        " \n",
        "    if not cap.isOpened():\n",
        "        print(\"could not open :\", filename)\n",
        "        exit(0)\n",
        " \n",
        "    return int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    #video_info('/content/drive/MyDrive/normal/sun3.mp4')\n",
        "\n",
        "    model = create_x3d(input_clip_length = 16, input_crop_size = 224, depth_factor = 2.2) # X3D_M\n",
        "    pretrained_path = \"/content/Extract-Features-X3D/X3D_M_extract_features.pth\"\n",
        "    model.load_state_dict(torch.load(pretrained_path), strict = False)\n",
        "    print(\"Load model successfully!!!\")\n",
        "\n",
        "    # Set to GPU or CPU\n",
        "    #device = \"cuda\"\n",
        "    model = model.eval()\n",
        "    #model = model.to(device)\n",
        "\n",
        "    #model.cuda()\n",
        "    \n",
        "    i = 0\n",
        "    dir_path = \"/content/drive/MyDrive/extra_resize/\"\n",
        "\n",
        "    for (root, directories, files) in os.walk(dir_path):\n",
        "      for file in files:\n",
        "          i = i+1\n",
        "          file_path = os.path.join(root, file)\n",
        "\n",
        "          video_paths = [file_path]\n",
        "\n",
        "          # Extract features\n",
        "          # 각 영상의\n",
        "          for video_path in video_paths:\n",
        "              print(video_path)\n",
        "              print(\"[\" + str(i) + \" / \" + \"37] , \" + str(i/37) + \"\\n\")\n",
        "\n",
        "              # save one video (1, 2048)\n",
        "              video = EncodedVideo.from_path(video_path)\n",
        "              start_sec = 0.0\n",
        "              stack_sec = 16/get_fps(video_path)\n",
        "              end_sec = stack_sec\n",
        "              count = (int)((get_count(video_path)) / 16)\n",
        "\n",
        "            # 한 segment (16 frame) 마다\n",
        "              for j in range(0, count):\n",
        "                end_sec += stack_sec * j\n",
        "                video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
        "                video_data = transform(video_data)\n",
        "                pre_x = video_data[\"video\"]\n",
        "                del video_data\n",
        "                x = pre_x.unsqueeze(0)\n",
        "                del pre_x\n",
        "\n",
        "                with torch.no_grad(): #autograd꺼서 메모리 사용량 줄이고 연산 속도 높임\n",
        "                  out = model(x)\n",
        "                del x\n",
        "                out2 = out.squeeze(dim=2)\n",
        "                del out\n",
        "                out3 = out2.squeeze(dim=2)\n",
        "                del out2\n",
        "                out4 = out3.squeeze(dim=2)\n",
        "                \n",
        "                print('one video percent : ' + str(j) + ' / ' + str(count))\n",
        "\n",
        "                savepath = \"/content/drive/MyDrive/extra_resize_result/\"\n",
        "                savepath += file_path[36:39]\n",
        "                savepath += '_'\n",
        "                savepath += str(j)\n",
        "                np.save(savepath, out4.tolist())\n",
        "                \n",
        "                torch.cuda.empty_cache()\n",
        "                del out4\n",
        "              del video\n",
        "          del file\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.1 (v3.10.1:2cd268a3a9, Dec  6 2021, 14:28:59) [Clang 13.0.0 (clang-1300.0.29.3)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
