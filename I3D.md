# I3D

Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset

Joa Ìƒo Carreiraâ€ , .. (2017)

<aside>
ğŸ’¡ Two_Stream Inflated 3D ConvNets(I3D) : RGB + Optical Flow ê°œë³„ ë„¤íŠ¸ì›Œí¬ ì˜ˆì¸¡ê°’ í‰ê· 

</aside>

<aside>
ğŸ’¡ Inflating : í•„í„°ì˜ dimension ëŠ˜ë¦¬ê³ , weight 1/N, Inception Block : ì—¬ëŸ¬ í¬ê¸° conv ë³‘ë ¬

</aside>

<aside>
ğŸ’¡ **ì´ë¯¸ì§€ì²˜ëŸ¼ ë¹„ë””ì˜¤ë„ transfer learning(ì „ì´ í•™ìŠµ, pre-trainedëœ ëª¨ë¸ì„ í•™ìŠµ)í•˜ë©´ ì´ë“ì´ ìˆê³ , íŠ¹íˆ kinetics dataset ì‚¬ìš©í•˜ë©´ ë„ì›€ì´ ëœë‹¤**

</aside>

```
Paper : [https://arxiv.org/abs/1507.05717](https://arxiv.org/abs/1507.05717)
code : [https://github.com/deepmind/kinetics-i3d](https://github.com/deepmind/kinetics-i3d) 
ì°¸ê³  ë¦¬ë·° 1 : [https://22-22.tistory.com/70](https://22-22.tistory.com/70)
ğŸ‘ì°¸ê³  ë¦¬ë·° 2 : [https://eremo2002.github.io/paperreview/I3D/](https://eremo2002.github.io/paperreview/I3D/)
ì°¸ê³  ë¦¬ë·° 3 : [https://junsk1016.github.io/deeplearning/i3D-%EB%A6%AC%EB%B7%B0/#introduction](https://junsk1016.github.io/deeplearning/i3D-%EB%A6%AC%EB%B7%B0/#introduction)
ì°¸ê³  ë¦¬ë·° 4 : [https://everyview.tistory.com/30](https://everyview.tistory.com/30) 
C3D->I3D ì—°êµ¬ë™í–¥ : [https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=khm159&logNo=222027509486](https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=khm159&logNo=222027509486)
í•œêµ­ì–´ë°œí‘œì˜ìƒ : [https://www.youtube.com/watch?v=kr5i-Wtf5Go](https://www.youtube.com/watch?v=kr5i-Wtf5Go) 
```

---

## 0.   Abstract

- **ìƒˆë¡œìš´ data set, kinetics**
    - ì†Œê·œëª¨ (UCF-101, HMDB-51) ì™€ ë‹¬ë¦¬ 400ê°œì˜ í–‰ë™, í´ë˜ìŠ¤ë‹¹ 400ê°œ ì´ìƒ í´ë¦½ì´ ìœ íŠœë¸Œì—ì„œ ìˆ˜ì§‘ë¨
    - kineticsë¡œ train í•˜ë©´ ì†Œê·œëª¨ data setì—ì„œ ì„±ëŠ¥ì´ ì–¼ë§ˆë‚˜ ì¢‹ì•„ì§€ë‚˜?
- **ìƒˆë¡œìš´ network, Two-Steram Inflated 3D ConvNet (I3D)**
    - *3D ConvNetì˜ ì‹œê³µê°„í™•ì¥ + Imagenet êµ¬ì¡°, íŒŒë¼ë¯¸í„° ì´ìš©*

## 1. Introduction

- ImageNetì„ í†µí•´ í° ë°ì´í„°ì…‹ìœ¼ë¡œ ì´ë¯¸ì§€ í›ˆë ¨í•˜ë©´ ë‹¤ë¥¸ êµ¬ì¡°ì—ë„ ì¨ë¨¹ëŠ”ë‹¤ ì•Œê²Œë¨. ì´ë¯¸ì§€ ë¿ ì•„ë‹ˆë¼ ë¹„ë””ì˜¤ë„ ê°™ì§€ ì•Šì„ê¹Œ? ë ˆì´ë¸” 400ê°œì— í´ë¦½ 400ê°œ ì´ìƒì˜ ìœ íŠœë¸Œ ì˜ìƒ, ë‘ë°° í° ë°ì´í„°ì…‹ kinetics ì´ìš©í•˜ì
- ì—¬ëŸ¬ net kineticsë¡œ í•™ìŠµí•œ í›„ ì†Œê·œëª¨ datasetì—ì„œ ë¯¸ì„¸ ì¡°ì •
    
    â‡’ ì‚¬ì „ í›ˆë ¨ì— ì˜í•œ ì„±ëŠ¥ í–¥ìƒì€ êµ¬ì¡° ë”°ë¼ ë‹¤ë¥´ë‹¤!
    
    â‡’ ì‚¬ì „ í›ˆë ¨ìœ¼ë¡œ ìµœê³ ì˜ ì„±ëŠ¥ ë½‘ì•„ë‚´ëŠ” ëª¨ë¸, Two-steam Inflated 3D ConvNets(I3D) : 
    
    **ì´ë¯¸ì§€ ë¶„ë¥˜, 3D Convë¡œ ì‹œê³µê°„** +  **Inception-v1ì— ê¸°ë°˜í•œ I3D ëª¨ë¸ë¡œ kineticsì— ëŒ€í•œ ì‚¬ì „ í›ˆë ¨ ìµœê³ ì˜ íš¨ê³¼** 
    

---

## 2. Action Classification Architectures

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2022-07-25 á„‹á…©á„’á…® 1.45.02.png](I3D%205b2e59b3e37240a596a78fd630b970a6/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-07-25_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_1.45.02.png)

- ë¹„ë””ì˜¤ ë¶„ì„ êµ¬ì¡° êµ¬ë¶„
    - Conv, layers operator : 2D(ì´ë¯¸ì§€ ê¸°ë°˜) or 3D(ë¹„ë””ì˜¤ ê¸°ë°˜)?
    - network input : RGB video or precomputed [optical flow](https://gaussian37.github.io/vision-concept-optical_flow/)(ê°ì²´ì›€ì§ì„)?
    - 2D convnetì´ë¼ë©´, ì •ë³´ê°€ [LSTM](https://dgkim5360.tistory.com/entry/understanding-long-short-term-memory-lstm-kr) ì‚¬ìš© or feature aggreation over time?
- ì´ë¯¸ì§€ ë¶„ë¥˜ ë„¤íŠ¸ì›Œí¬ (Inception, ResNet)ë„ ì‹œê³µê°„ìœ¼ë¡œ í™•ì¥(Infate)ê°€ëŠ¥
    
    â‡’ ì‚¬ì „ í•™ìŠµëœ ImageNet êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Inception-v1 batch normalization í•˜ë©´ êµ¬ë¶„ ì˜ ë  ê²ƒ!
    

### 1) The Old 1 : ConvNet + LSTM

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2022-07-25 á„‹á…©á„Œá…¥á†« 10.12.02.png](I3D%205b2e59b3e37240a596a78fd630b970a6/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-07-25_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_10.12.02.png)

- LSTM
    - **í”„ë ˆì„ë³„ ë…ë¦½ì  íŠ¹ì§• ì¶”ì¶œì€ ì‹œê°„ì  êµ¬ì¡°ë¥¼ ë¬´ì‹œ**í•¨. â‡’ ë°˜ë³µ ê³„ì¸µì„ ì¶”ê°€í•´ ìƒíƒœì™€ ì‹œê°„ ìˆœì„œ, ì¥ê±°ë¦¬ ì¢…ì†ì„±ì„ ê°€ì ¸ì•¼ í•¨.
- LSTM + ConvNet
    - Inception-V1ì˜ ë§ˆì§€ë§‰ avarege pooling layer ë’¤ì— batch normì„ ê°€ì§„ LSTMë°°ì¹˜
    - ë§¨ìœ„ì— fc layer ì¶”ê°€
    - í”„ë ˆì„ 5/25, ë§ˆì§€ë§‰ ê²°ê³¼ í”„ë ˆì„ë§Œ, [cross-entropy-losses](https://wandb.ai/wandb_fc/korean/reports/---VmlldzoxNDI4NDUx)(ë°œê²¬ëœ í™•ë¥ ë¶„í¬ - ì˜ˆì¸¡í•œ í™•ë¥ ë¶„í¬)ë¡œ í…ŒìŠ¤íŠ¸
    - ë†’ì€ ìˆ˜ì¤€ ë³€í™” o, ë¯¸ì„¸í•œ **ë‚®ì€ ìˆ˜ì¤€ ì›€ì§ì„** x
    - [Backpropagation-through-time](http://solarisailab.com/archives/1451)(ì—ëŸ¬í•©)ìœ„í•´ ì—¬ëŸ¬ í”„ë ˆì„ì˜ ë„¤íŠ¸ì›Œí¬ í¼ì³ì•¼ â‡’**train ë¹„ìš© ë§**ì´ë“¦

### 2) The Old 2 : 3D ConvNets

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2022-07-25 á„‹á…©á„Œá…¥á†« 10.11.08.png](I3D%205b2e59b3e37240a596a78fd630b970a6/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-07-25_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_10.11.08.png)

- C3D : ì‹œê³µê°„ í•„í„°ê°€ ìˆëŠ” í‘œì¤€ convnetê³¼ ë¹„ìŠ·í•œ ë¹„ë””ì˜¤ ëª¨ë¸ë§
    - **ì‹œê³µê°„ ë°ì´í„°ì˜ ê³„ì¸µì  í‘œí˜„ì„ ì§ì ‘ ë§Œë“¦**
    - **conv ì°¨ì› ì¶”ê°€ë¡œ parameter ëŠ˜ì–´ train ì–´ë ¤ì›€**
    - **ì‚¬ì „ í›ˆë ¨ ì´ì  x â‡’ ì–•ì€ êµ¬ì¡°ë¡œ ì²˜ìŒë¶€í„° trainí•´ì•¼**
- kinetics í‰ê°€ ìœ„í•´ **ì›í˜• ëª¨ë¸ì—ì„œ ë³€í˜•**
    
    ![IMG_38044EE941D1-1.jpeg](I3D%205b2e59b3e37240a596a78fd630b970a6/IMG_38044EE941D1-1.jpeg)
    
    - ëª¨ë“  conv, fc ë’¤ì— batch norm
    - ì²« poolingì—ì„œ kernel depth 1â†’2 (ì‹œê°„ ì‹ ê²½ì”€)
    
    â‡’ í‘œì¤€ K40 GPU ì‚¬ìš©, ë°°ì¹˜ë‹¹ 15ê°œ ë¹„ë””ì˜¤ë¡œ êµìœ¡ ã„±ã„´
    

### 3) The Old 3 : Two-Stream Networks

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2022-07-25 á„‹á…©á„Œá…¥á†« 10.29.14.png](I3D%205b2e59b3e37240a596a78fd630b970a6/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-07-25_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_10.29.14.png)

- **Two-Stream(ì´ì „)**
    - ImageNetìœ¼ë¡œ Pre-trainëœ ConvNet ë‘ê°œ ë³µì œë³¸ í†µê³¼ â‡’  1ê°œì˜ **RGB frame** **+** 10ê°œì˜ ê²‰ìœ¼ë¡œ ê³„ì‚°ëœ **opticalflow**(ê°ì²´ì›€ì§ì„) frameì˜ ìŠ¤íƒìœ¼ë¡œë¶€í„° ì˜ˆì¸¡ í‰ê·  â‡’ ë¹„ë””ì˜¤ì˜ ì§§ì€ ì‹œê°„ ìŠ¤ëƒ…ìƒ· ëª¨ë¸ë§
    - flow streamì€ flow frameë³´ë‹¤ input chanelì´ 2ë°°(ìˆ˜í‰, ìˆ˜ì§) ë” ë§ì€ adaptive input conv layer â‡’ ì—¬ëŸ¬ ìŠ¤ëƒ…ìƒ· ìƒ˜í”Œë§, ì˜ˆì¸¡ í‰ê· í™” â‡’ **ë†’ì€ ì„±ëŠ¥, train, test íš¨ìœ¨**
- **3D-Fused Two-Stream(ìµœê·¼)**

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2022-07-25 á„‹á…©á„Œá…¥á†« 11.20.58.png](I3D%205b2e59b3e37240a596a78fd630b970a6/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-07-25_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_11.20.58.png)

- ë§¨ë’¤ conv layer ë’¤ì—ì„œ ì‹œê³µê°„ + flow stream í•©ì¹¨ â‡’ í…ŒìŠ¤íŠ¸ ì‹œê°„ ì¤„ì„, HMDB ì„±ëŠ¥ë†’
- **í‰ê°€ ìœ„í•´ ë³€í˜•**
    - Inception-V1 ì‚¬ìš©, 10í”„ë ˆì„ ê°„ê²© ì—°ì† RGB frame(optical flowë‘ ë¹„ìŠ·í•˜ê²Œ)
    - 3*3*3conv layer(512ê°œì¶œë ¥) â‡’ 3*3 mac pooling layer â‡’ fc layer â‡’ Inception-V1 ë§¨ë’¤ average pooling layer(5*7*7)
    - Gaussian noiseë¡œ ì´ˆê¸°í™”
    - ëª¨ë‘ [end-to-endí•™ìŠµ(ì¢…ë‹¨ê°„í•™ìŠµ, ì…ë ¥ì¶œë ¥í•œë²ˆì—)](https://velog.io/@jeewoo1025/What-is-end-to-end-deep-learning)

### 4) The New : Two-Steam Inflated 3D ConvNets

C3Dê°€ ImageNet 2D ConvNet ì„¤ê³„ + ì„ íƒì  trainëœ Parameterë¡œë¶€í„° ì–´ë–¤ ì´ë“? 

C3Dì— RGB + optical flow(ì§ì ‘ ì‹œê°„ íŒ¨í„´ í•™ìŠµ ëŒ€ì‹ )ë¡œ ì„±ëŠ¥ í–¥ìƒ 

- **Inflating 2D ConvNets into 3D**
    - spatio-temporal model ë°˜ë³µ â‡’ ëª¨ë¸ ìì²´ë¥¼ 2Dì—ì„œ 3Dë¡œ
    - 2D êµ¬ì¡°ì—ì„œ ëª¨ë“  **filter, polling kernal í™•ì¥(inflate)** : ì‹œê°„ ì°¨ì›ì¶”ê°€
    - NxN filter â‡’ N*N*N filter
- **Bootstrapping 3D filters from 2D filters**
    - 2Dâ‡’3D êµ¬ì¡° ê·¸ëŒ€ë¡œ filter í™•ì¥ â‡’ pre-trained ImageNet model íŒŒë¼ë¯¸í„° ì‚¬ìš©
    - 3D filterì€ 2D filterì— ì‹œê°„ ì¶• ì¶”ê°€í•´ í™•ì¥í•œ ê²ƒ(boring) â‡’**2D filterì˜ weightê°’ì„ 3D filterì˜ ì‹œê°„ ì¶• ë”°ë¼ ë³µì‚¬í•˜ê³  ì´ë¥¼ 1/Në¡œ rescaling**
- **Pacing receptive field growth in space, time and net-work depth**
    - videoëŠ” ì¸ì ‘í•œ frameë¼ë¦¬ ìœ ì‚¬ â‡’ ëª¨ë“  frameì´ ë…ë¦½ì , ìœ ì˜ë¯¸ëŠ” x
    - poolingê³¼ conv layerì—ì„œ temporal stride ì–¼ë§ˆë¡œ ì¤„ ê²ƒì¸ì§€ ì¤‘ìš”
        - ì‹œê°„ì •ë³´ê°€ ê³µê°„ ë¹„í•´ ë” ë¹ ë¥´ê²Œ ê°ì§€ë˜ë©´ : ì´ˆê¸° íŠ¹ì§• ê°ì§€ x
        - ì‹œê°„ì •ë³´ê°€ ê³µê°„ ë¹„í•´ ë” ëŠë¦¬ê²Œ ê°ì§€ë˜ë©´ : ì¥ë©´ ì—­í•  ê°ì§€ x
    - ì‹¤í—˜ì ìœ¼ë¡œ **ì²« ë‘ê°œì˜ max poolingì—ì„œëŠ” ì‹œê°„ xê°€ íš¨ê³¼ì **ì„ì„ ë°œê²¬
        
        ![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2022-07-25 á„‹á…©á„’á…® 1.26.37.png](I3D%205b2e59b3e37240a596a78fd630b970a6/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-07-25_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_1.26.37.png)
        
    - **ì¦‰, Inceptionì€ ì—¬ëŸ¬ ê°œì˜ convolution layer ë³‘ë ¬ì ìœ¼ë¡œ ì‚¬ìš©í•´ ì—°ì‚°ëŸ‰ ì¡°ì ˆ**
        
        ![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2022-07-25 á„‹á…©á„’á…® 1.17.41.png](I3D%205b2e59b3e37240a596a78fd630b970a6/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-07-25_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_1.17.41.png)
        
    - [Inception-V1(GoogleNet)](https://oi.readthedocs.io/en/latest/computer_vision/cnn/googlenet.html) : Sparse network and dense matrix
    - ê·¸ë˜ì„œ ì´ë ‡ê²Œ max-pool ì²« ë‘ê°œ stride 1,2,2(ì»¤ë„í¬ê¸°ëŠ” 1*3*3), ìµœì¢… average pooling layerì´ 2*7*7ì»¤ë„ì´ ë˜ê²Œë”
- **Two 3D Streams**
    
    ![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2022-07-25 á„‹á…©á„’á…® 1.25.38.png](I3D%205b2e59b3e37240a596a78fd630b970a6/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-07-25_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_1.25.38.png)
    
    - I3DëŠ” **RGBë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ” ë„¤íŠ¸ì›Œí¬**ì™€, **optical flow(ê°ì²´ì›€ì§ì„)ì„ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ” ë„¤íŠ¸ì›Œí¬** ë‘ ê°œë¡œ ì´ë£¨ì–´ì§ : two stream network
    - ë‘ ë„¤íŠ¸ì›Œí¬ **ê°œë³„ í•™ìŠµ**, ë‘ **ì˜ˆì¸¡ê²°ê³¼ average**

### 5) Implementation Details

- ì‹¤í—˜ì—ì„œ
    - C3D ì œì™¸, ëª¨ë‘ Imagenetìœ¼ë¡œ pre-trained Inceptopn-v1 ê¸°ë³¸ ë„¤íŠ¸ì›Œí¬ë¡œ
    - ëª¨ë“  êµ¬ì¡°ì—ì„œ ë§ˆì§€ë§‰ conv layerì œì™¸í•˜ê³  batch normê³¼ reluí•¨
    - 0.9 [SGC(Stochastic Gradient Descent, í™•ë¥ ê²½ì‚¬í•˜ê°•ë²•, ë¬´ì‘ìœ„ ì˜ˆì˜ ì˜ˆì¸¡ ê²½ì‚¬ ê³„ì‚°)](https://everyday-deeplearning.tistory.com/entry/SGD-Stochastic-Gradient-Descent-%ED%99%95%EB%A5%A0%EC%A0%81-%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95)
    - 32ê°œ GPU ë³‘ë ¬í™”
    - kinetics ë°ì´í„° ì„¸íŠ¸, tensorflow
    - ê³µê°„ì  ë¬´ì‘ìœ„ ìë¥´ê¸° (256*256â‡’ 224*224), ë¹„ë””ì˜¤ ë£¨í”„, ì¢Œìš° ë’¤ì§‘ê¸°, ì¢Œìš°ë°˜ì „, ì¸¡ê´‘
    - optical flow : TV-L1(Total Variation L1, ë…¸ì´ì¦ˆ ì œê±°í•´ì„œ ê°ì²´ í–‰ë™)

---

## 3. The Kinetics Human Action Video Dataset

- í™œë™, ì‚¬ê±´ë³´ë‹¤ í–‰ë™ì— ì´ˆì 
- ì„¸ë¶„í™”ëœ ë™ì‘ì€ ì‹œê°„ì  ì¶”ë¡ , ëŒ€ìƒ ê°•ì¡° í•„ìš”
- 400ê°œ í´ë˜ìŠ¤, ê° í´ë˜ìŠ¤ ë§ˆë‹¤ 400ê°œ ì´ìƒì˜ clip, ê° í´ë˜ìŠ¤ë§ˆë‹¤ ì´ 240000ê°œì˜ trian video. 10ì´ˆ, ëª¨ë‘ ë‹¤ë“¬ì–´ì§
- test setì€ ê° í´ë˜ìŠ¤ë³„ 100ê°œ clip

---

## 4. Experimental Comparison of Architecture

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2022-07-25 á„‹á…©á„’á…® 1.46.28.png](I3D%205b2e59b3e37240a596a78fd630b970a6/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-07-25_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_1.46.28.png)

- **I3Dê°€ ê°€ì¥ ì˜ ìˆ˜í–‰ëœë‹¤. RGB + optical flow ê°™ì´ ì“¸ë•Œ ì„±ëŠ¥ ê°€ì¥ ì¢‹ë‹¤**
- ëª¨ë‘ UCF-101ë³´ë‹¤ kineticsê°€ ì„±ëŠ¥ ë‚®ë‹¤
    - HMDB-51 : train data ë¶€ì¡±, ì˜ë„ì ìœ¼ë¡œ ì–´ë ¤ì›€
- êµ¬ì¡° ìˆœìœ„ dataset ë§ˆë‹¤ ì¼ê´€ì 
    - kineticsì™€ ë‹¬ë¦° UCF, HMDBì—ì„œëŠ” RGB ë³´ë‹¤ optical flowê°€ ë” ë‚«ë‹¤ : kineticsì—ëŠ” ì¹´ë©”ë¼ motionì´ ë‹¤ì–‘í•´ motion stream í•™ìŠµ ì–´ë µê¸° ë•Œë¬¸

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2022-07-25 á„‹á…©á„’á…® 1.53.38.png](I3D%205b2e59b3e37240a596a78fd630b970a6/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-07-25_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_1.53.38.png)

- **imagenetìœ¼ë¡œ per-trainedí•œ ëª¨ë¸ì´ ë” ì¢‹ë‹¤**

---

## 5. Experimental Evaluation of Features

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2022-07-25 á„‹á…©á„’á…® 1.58.40.png](I3D%205b2e59b3e37240a596a78fd630b970a6/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-07-25_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_1.58.40.png)

- pretrained ì‚¬ìš©o / ì‚¬ìš©x
- origin / fixed / full-ft
    - origin : UCF í˜¹ì€ HMDBì—ì„œ í›ˆë ¨ë¨
    - fixed : kinetics featureì¸ë°, ë§ˆì§€ë§‰ layerë§Œ UCFë‚˜ HMDBì—ì„œ í›ˆë ¨ë¨
    - full-ft : UCFë‚˜ HMDB ë¯¸ì„¸ì¡°ì •í›„ end-to-end í•™ìŠµí•œ kineticsë¡œ í›ˆë ¨ë¨
- ëª¨ë“  ë°ì´í„°, íŠ¹íˆ I3Dì™€ C3Dê°€ kinetics pre-trained(â†) ì´ë“ì–»ìŒ.
    - I3DëŠ” high temporal resolution ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— íŠ¹íˆ êµ¿. fps25ì¸ ë¹„ë””ì˜¤ì—ì„œ 64frame ìƒ˜í”Œë§í•˜ê¸° ë•Œë¬¸ì— fine-grained temporal action feature(í”„ë¡œì„¸ìŠ¤ ë‚˜ëˆ ì„œ ì‹œê°„ì  ë™ì‘ ì •ë³´) ì˜ ì¶”ì¶œí•  ìˆ˜ ìˆìŒ. ì´ì™€ ë‹¬ë¦¬ frame samplingì´ ì ì€ ê²½ìš° kinetics datasetìœ¼ë¡œ pre-trainingì‹œì¼°ì„ ë•Œ íš¨ê³¼ê°€ í¬ì§€ ì•ŠìŒ
    - two-streamì€ pre-training íš¨ê³¼ ë‚«êµ¿
        - flow streamì´ ì´ë¯¸ ìˆì–´ì„œ overfitting ì•ˆë¼ ì •í™•ë„ êµ¿
- **kineticsë¡œ pre-trainingí•˜ëŠ” ê²ƒì´ imagenetìœ¼ë¡œ pre-trainingí•˜ëŠ” ê²ƒ ë³´ë‹¤ ë‚«ë‹¤**

### 1) Comparision with the State-of-the-Art

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2022-07-25 á„‹á…©á„’á…® 2.20.58.png](I3D%205b2e59b3e37240a596a78fd630b970a6/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-07-25_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_2.20.58.png)

- Two-Stream I3Dê°€ ì´ê²¼ë”°
- kinetics pre-trained I3Dê°€ sports 1m pre-trained C3D ì´ê²¼ë”°
    - **I3Dêµ¬ì¡°, kinetics datasetì´ ë” ì„ë‹¤**

---

## 6. Discussion

- ì´ë¯¸ì§€ë„·ì²˜ëŸ¼ ë¹„ë””ì˜¤ ë„ë©”ì¸ì—ì„œë„ ì „ì´í•™ìŠµ(transfer learning, pre-trainedëœ ëª¨ë¸í•™ìŠµ)í•˜ë©´ ì´ë“ ìˆì„ê¹Œ? â‡’ kinetics ì´ìš©í•˜ë©´ ì´ë“
    - **kinetics datasetìœ¼ë¡œ pre-trainingí•˜ê³  ë‹¤ë¥¸ ë°ì´í„°ì…‹ fine-tuning í•˜ë©´ êµ¿**
- ê·¸ëŸ¬ë‚˜ kineticsë¡œ pre-trainingí•˜ëŠ”ê²ƒì´ video segmentation, video object detection, optical flow computation ë“± ë‹¤ë¥¸ ë¹„ë””ì˜¤ taskì—ì„œë„ ì´ë“ì¸ì§€ëŠ” ëª°?ë£¨
- I3Dë¼ëŠ” íš¨ê³¼ì ì¸ ëª¨ë¸
    - comprehensive exploration ìˆ˜í–‰ x
    - action tubeë‚˜ attention mechanismê°™ì´ human actorì— focus í•  ìˆ˜ ìˆëŠ” í…Œí¬ë‹‰ ì ìš© x
    - spaceì™€ time ê°„ì˜ ê´€ê³„ ë¶„ì„í•˜ê³  ì¤‘ìš”í•œ ì •ë³´ë§Œ ê°ì§€í•˜ëŠ” ê¸°ìˆ ë¡œ ì¶”ê°€ ì—°êµ¬ í•„ìš”

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable

import numpy as np

import os
import sys
from collections import OrderedDict

class MaxPool3dSamePadding(nn.MaxPool3d):
    
    def compute_pad(self, dim, s):
        if s % self.stride[dim] == 0:
            return max(self.kernel_size[dim] - self.stride[dim], 0)
        else:
            return max(self.kernel_size[dim] - (s % self.stride[dim]), 0)

    def forward(self, x):
        # compute 'same' padding
        (batch, channel, t, h, w) = x.size()
        #print t,h,w
        out_t = np.ceil(float(t) / float(self.stride[0]))
        out_h = np.ceil(float(h) / float(self.stride[1]))
        out_w = np.ceil(float(w) / float(self.stride[2]))
        #print out_t, out_h, out_w
        pad_t = self.compute_pad(0, t)
        pad_h = self.compute_pad(1, h)
        pad_w = self.compute_pad(2, w)
        #print pad_t, pad_h, pad_w

        pad_t_f = pad_t // 2
        pad_t_b = pad_t - pad_t_f
        pad_h_f = pad_h // 2
        pad_h_b = pad_h - pad_h_f
        pad_w_f = pad_w // 2
        pad_w_b = pad_w - pad_w_f

        pad = (pad_w_f, pad_w_b, pad_h_f, pad_h_b, pad_t_f, pad_t_b)
        #print x.size()
        #print pad
        x = F.pad(x, pad)
        return super(MaxPool3dSamePadding, self).forward(x)
    

class Unit3D(nn.Module):

    def __init__(self, in_channels,
                 output_channels,
                 kernel_shape=(1, 1, 1),
                 stride=(1, 1, 1),
                 padding=0,
                 activation_fn=F.relu,
                 use_batch_norm=True,
                 use_bias=False,
                 name='unit_3d'):
        
        """Initializes Unit3D module."""
        super(Unit3D, self).__init__()
        
        self._output_channels = output_channels
        self._kernel_shape = kernel_shape
        self._stride = stride
        self._use_batch_norm = use_batch_norm
        self._activation_fn = activation_fn
        self._use_bias = use_bias
        self.name = name
        self.padding = padding
        
        self.conv3d = nn.Conv3d(in_channels=in_channels,
                                out_channels=self._output_channels,
                                kernel_size=self._kernel_shape,
                                stride=self._stride,
                                padding=0, # we always want padding to be 0 here. We will dynamically pad based on input size in forward function
                                bias=self._use_bias)
        
        if self._use_batch_norm:
            self.bn = nn.BatchNorm3d(self._output_channels, eps=0.001, momentum=0.01)

    def compute_pad(self, dim, s):
        if s % self._stride[dim] == 0:
            return max(self._kernel_shape[dim] - self._stride[dim], 0)
        else:
            return max(self._kernel_shape[dim] - (s % self._stride[dim]), 0)

            
    def forward(self, x):
        # compute 'same' padding
        (batch, channel, t, h, w) = x.size()
        #print t,h,w
        out_t = np.ceil(float(t) / float(self._stride[0]))
        out_h = np.ceil(float(h) / float(self._stride[1]))
        out_w = np.ceil(float(w) / float(self._stride[2]))
        #print out_t, out_h, out_w
        pad_t = self.compute_pad(0, t)
        pad_h = self.compute_pad(1, h)
        pad_w = self.compute_pad(2, w)
        #print pad_t, pad_h, pad_w

        pad_t_f = pad_t // 2
        pad_t_b = pad_t - pad_t_f
        pad_h_f = pad_h // 2
        pad_h_b = pad_h - pad_h_f
        pad_w_f = pad_w // 2
        pad_w_b = pad_w - pad_w_f

        pad = (pad_w_f, pad_w_b, pad_h_f, pad_h_b, pad_t_f, pad_t_b)
        #print x.size()
        #print pad
        x = F.pad(x, pad)
        #print x.size()        

        x = self.conv3d(x)
        if self._use_batch_norm:
            x = self.bn(x)
        if self._activation_fn is not None:
            x = self._activation_fn(x)
        return x

# *** InceptionModule ***
class InceptionModule(nn.Module):
    def __init__(self, in_channels, out_channels, name):
        super(InceptionModule, self).__init__()

        self.b0 = Unit3D(in_channels=in_channels, output_channels=out_channels[0], kernel_shape=[1, 1, 1], padding=0,
                         name=name+'/Branch_0/Conv3d_0a_1x1')
        self.b1a = Unit3D(in_channels=in_channels, output_channels=out_channels[1], kernel_shape=[1, 1, 1], padding=0,
                          name=name+'/Branch_1/Conv3d_0a_1x1')
        self.b1b = Unit3D(in_channels=out_channels[1], output_channels=out_channels[2], kernel_shape=[3, 3, 3],
                          name=name+'/Branch_1/Conv3d_0b_3x3')
        self.b2a = Unit3D(in_channels=in_channels, output_channels=out_channels[3], kernel_shape=[1, 1, 1], padding=0,
                          name=name+'/Branch_2/Conv3d_0a_1x1')
        self.b2b = Unit3D(in_channels=out_channels[3], output_channels=out_channels[4], kernel_shape=[3, 3, 3],
                          name=name+'/Branch_2/Conv3d_0b_3x3')
        self.b3a = MaxPool3dSamePadding(kernel_size=[3, 3, 3],
                                stride=(1, 1, 1), padding=0)
        self.b3b = Unit3D(in_channels=in_channels, output_channels=out_channels[5], kernel_shape=[1, 1, 1], padding=0,
                          name=name+'/Branch_3/Conv3d_0b_1x1')
        self.name = name

    def forward(self, x):    
        b0 = self.b0(x)
        b1 = self.b1b(self.b1a(x))
        b2 = self.b2b(self.b2a(x))
        b3 = self.b3b(self.b3a(x))
        return torch.cat([b0,b1,b2,b3], dim=1)

class InceptionI3d(nn.Module):
    """Inception-v1 I3D architecture.
    The model is introduced in:
        Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset
        Joao Carreira, Andrew Zisserman
        https://arxiv.org/pdf/1705.07750v1.pdf.
    See also the Inception architecture, introduced in:
        Going deeper with convolutions
        Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,
        Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.
        http://arxiv.org/pdf/1409.4842v1.pdf.
    """

    # Endpoints of the model in order. During construction, all the endpoints up
    # to a designated `final_endpoint` are returned in a dictionary as the
    # second return value.
    VALID_ENDPOINTS = (
        'Conv3d_1a_7x7',
        'MaxPool3d_2a_3x3',
        'Conv3d_2b_1x1',
        'Conv3d_2c_3x3',
        'MaxPool3d_3a_3x3',
        'Mixed_3b',
        'Mixed_3c',
        'MaxPool3d_4a_3x3',
        'Mixed_4b',
        'Mixed_4c',
        'Mixed_4d',
        'Mixed_4e',
        'Mixed_4f',
        'MaxPool3d_5a_2x2',
        'Mixed_5b',
        'Mixed_5c',
        'Logits',
        'Predictions',
    )

    def __init__(self, num_classes=400, spatial_squeeze=True,
                 final_endpoint='Logits', name='inception_i3d', in_channels=3, dropout_keep_prob=0.5):
        """Initializes I3D model instance.
        Args:
          num_classes: The number of outputs in the logit layer (default 400, which
              matches the Kinetics dataset).
          spatial_squeeze: Whether to squeeze the spatial dimensions for the logits
              before returning (default True).
          final_endpoint: The model contains many possible endpoints.
              `final_endpoint` specifies the last endpoint for the model to be built
              up to. In addition to the output at `final_endpoint`, all the outputs
              at endpoints up to `final_endpoint` will also be returned, in a
              dictionary. `final_endpoint` must be one of
              InceptionI3d.VALID_ENDPOINTS (default 'Logits').
          name: A string (optional). The name of this module.
        Raises:
          ValueError: if `final_endpoint` is not recognized.
        """

        if final_endpoint not in self.VALID_ENDPOINTS:
            raise ValueError('Unknown final endpoint %s' % final_endpoint)

        super(InceptionI3d, self).__init__()
        self._num_classes = num_classes
        self._spatial_squeeze = spatial_squeeze
        self._final_endpoint = final_endpoint
        self.logits = None

        if self._final_endpoint not in self.VALID_ENDPOINTS:
            raise ValueError('Unknown final endpoint %s' % self._final_endpoint)

        self.end_points = {}
        end_point = 'Conv3d_1a_7x7'
        self.end_points[end_point] = Unit3D(in_channels=in_channels, output_channels=64, kernel_shape=[7, 7, 7],
                                            stride=(2, 2, 2), padding=(3,3,3),  name=name+end_point)
        if self._final_endpoint == end_point: return
        
        end_point = 'MaxPool3d_2a_3x3'
        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[1, 3, 3], stride=(1, 2, 2),
                                                             padding=0)
        if self._final_endpoint == end_point: return
        
        end_point = 'Conv3d_2b_1x1'
        self.end_points[end_point] = Unit3D(in_channels=64, output_channels=64, kernel_shape=[1, 1, 1], padding=0,
                                       name=name+end_point)
        if self._final_endpoint == end_point: return
        
        end_point = 'Conv3d_2c_3x3'
        self.end_points[end_point] = Unit3D(in_channels=64, output_channels=192, kernel_shape=[3, 3, 3], padding=1,
                                       name=name+end_point)
        if self._final_endpoint == end_point: return

        end_point = 'MaxPool3d_3a_3x3'
        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[1, 3, 3], stride=(1, 2, 2),
                                                             padding=0)
        if self._final_endpoint == end_point: return
        
        end_point = 'Mixed_3b'
        self.end_points[end_point] = InceptionModule(192, [64,96,128,16,32,32], name+end_point)
        if self._final_endpoint == end_point: return

        end_point = 'Mixed_3c'
        self.end_points[end_point] = InceptionModule(256, [128,128,192,32,96,64], name+end_point)
        if self._final_endpoint == end_point: return

        end_point = 'MaxPool3d_4a_3x3'
        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(2, 2, 2),
                                                             padding=0)
        if self._final_endpoint == end_point: return

        end_point = 'Mixed_4b'
        self.end_points[end_point] = InceptionModule(128+192+96+64, [192,96,208,16,48,64], name+end_point)
        if self._final_endpoint == end_point: return

        end_point = 'Mixed_4c'
        self.end_points[end_point] = InceptionModule(192+208+48+64, [160,112,224,24,64,64], name+end_point)
        if self._final_endpoint == end_point: return

        end_point = 'Mixed_4d'
        self.end_points[end_point] = InceptionModule(160+224+64+64, [128,128,256,24,64,64], name+end_point)
        if self._final_endpoint == end_point: return

        end_point = 'Mixed_4e'
        self.end_points[end_point] = InceptionModule(128+256+64+64, [112,144,288,32,64,64], name+end_point)
        if self._final_endpoint == end_point: return

        end_point = 'Mixed_4f'
        self.end_points[end_point] = InceptionModule(112+288+64+64, [256,160,320,32,128,128], name+end_point)
        if self._final_endpoint == end_point: return

        end_point = 'MaxPool3d_5a_2x2'
        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[2, 2, 2], stride=(2, 2, 2),
                                                             padding=0)
        if self._final_endpoint == end_point: return

        end_point = 'Mixed_5b'
        self.end_points[end_point] = InceptionModule(256+320+128+128, [256,160,320,32,128,128], name+end_point)
        if self._final_endpoint == end_point: return

        end_point = 'Mixed_5c'
        self.end_points[end_point] = InceptionModule(256+320+128+128, [384,192,384,48,128,128], name+end_point)
        if self._final_endpoint == end_point: return

        end_point = 'Logits'
        self.avg_pool = nn.AvgPool3d(kernel_size=[2, 7, 7],
                                     stride=(1, 1, 1))
        self.dropout = nn.Dropout(dropout_keep_prob)
        self.logits = Unit3D(in_channels=384+384+128+128, output_channels=self._num_classes,
                             kernel_shape=[1, 1, 1],
                             padding=0,
                             activation_fn=None,
                             use_batch_norm=False,
                             use_bias=True,
                             name='logits')

        self.build()

    def replace_logits(self, num_classes):
        self._num_classes = num_classes
        self.logits = Unit3D(in_channels=384+384+128+128, output_channels=self._num_classes,
                             kernel_shape=[1, 1, 1],
                             padding=0,
                             activation_fn=None,
                             use_batch_norm=False,
                             use_bias=True,
                             name='logits')
        
    
    def build(self):
        for k in self.end_points.keys():
            self.add_module(k, self.end_points[k])
        
    def forward(self, x):
        for end_point in self.VALID_ENDPOINTS:
            if end_point in self.end_points:
                x = self._modules[end_point](x) # use _modules to work with dataparallel

        x = self.logits(self.dropout(self.avg_pool(x)))
        if self._spatial_squeeze:
            logits = x.squeeze(3).squeeze(3)
        # logits is batch X time X classes, which is what we want to work with
        return logits
        

    def extract_features(self, x):
        for end_point in self.VALID_ENDPOINTS:
            if end_point in self.end_points:
                x = self._modules[end_point](x)
        return self.avg_pool(x)
```