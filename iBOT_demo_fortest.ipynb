{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5Q9onkzwNIh"
      },
      "source": [
        "# VideoTransformer\n",
        "\n",
        "TimeSformer(https://arxiv.org/abs/2102.05095), ViViT(https://arxiv.org/abs/2103.15691)\n",
        "\n",
        "Welcome to the demo notebook for VideoTransformer. We'll showcase the prediction result by the above pre-trained models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qm1zeSI4wNIk"
      },
      "source": [
        "## Preliminaries\n",
        "\n",
        "This section contains initial setup. Run it first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gv1qdarnNNVE",
        "outputId": "b21f483e-7f9f-4d92-f716-e1432e59492e",
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchvision) (4.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.13.1+cu116)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.25.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (4.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: decord in /usr/local/lib/python3.8/dist-packages (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from decord) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.8/dist-packages (0.6.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.8/dist-packages (0.18.3)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (1.21.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (2022.10.10)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (2.9.0)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (3.2.2)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (7.1.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (3.0)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (1.7.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.8/dist-packages (1.8.6)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (2022.11.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (6.0)\n",
            "Requirement already satisfied: tensorboardX>=2.2 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (2.5.1)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (1.13.1+cu116)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (1.21.6)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (0.11.0)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (4.64.1)\n",
            "Requirement already satisfied: lightning-utilities!=0.4.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (0.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.25.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=17.0->pytorch-lightning) (3.0.9)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX>=2.2->pytorch-lightning) (3.19.6)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.1.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (22.2.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install --user torch\n",
        "!pip3 install --user torchvision\n",
        "!pip3 install --user matplotlib\n",
        "!pip3 install --user decord\n",
        "!pip3 install --user einops\n",
        "!pip3 install --user scikit-image\n",
        "!pip3 install --user pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sJv3G2wwzhT",
        "outputId": "8029505b-ee27-480c-81b8-5f265d34f1b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: einops 0.6.0\n",
            "Uninstalling einops-0.6.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.8/dist-packages/einops-0.6.0.dist-info/*\n",
            "    /usr/local/lib/python3.8/dist-packages/einops/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled einops-0.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Using cached einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall einops\n",
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tBBG_T32pzPH",
        "outputId": "994c31d3-e9f9-48d7-920d-9c7652c39d0f",
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'VideoTransformer-pytorch'...\n",
            "remote: Enumerating objects: 159, done.\u001b[K\n",
            "remote: Counting objects: 100% (120/120), done.\u001b[K\n",
            "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
            "remote: Total 159 (delta 61), reused 100 (delta 48), pack-reused 39\u001b[K\n",
            "Receiving objects: 100% (159/159), 4.26 MiB | 8.07 MiB/s, done.\n",
            "Resolving deltas: 100% (69/69), done.\n",
            "/content/VideoTransformer-pytorch/VideoTransformer-pytorch/VideoTransformer-pytorch\n",
            "Found existing installation: decord 0.6.0\n",
            "Uninstalling decord-0.6.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.8/dist-packages/decord-0.6.0.dist-info/*\n",
            "    /usr/local/lib/python3.8/dist-packages/decord.libs/libXau-b2e5323c.so.6.0.0\n",
            "    /usr/local/lib/python3.8/dist-packages/decord.libs/libavcodec-bc50294c.so.58.35.100\n",
            "    /usr/local/lib/python3.8/dist-packages/decord.libs/libavdevice-bf61e037.so.58.5.100\n",
            "    /usr/local/lib/python3.8/dist-packages/decord.libs/libavfilter-1e2243e2.so.7.40.101\n",
            "    /usr/local/lib/python3.8/dist-packages/decord.libs/libavformat-8b46ea57.so.58.20.100\n",
            "    /usr/local/lib/python3.8/dist-packages/decord.libs/libavutil-2b26904a.so.56.22.100\n",
            "    /usr/local/lib/python3.8/dist-packages/decord.libs/libbz2-13e8c345.so.1.0.4\n",
            "    /usr/local/lib/python3.8/dist-packages/decord.libs/libpostproc-88b722f8.so.55.3.100\n",
            "    /usr/local/lib/python3.8/dist-packages/decord.libs/libswresample-230d9f46.so.3.3.100\n",
            "    /usr/local/lib/python3.8/dist-packages/decord.libs/libswscale-8e37dcfd.so.5.3.100\n",
            "    /usr/local/lib/python3.8/dist-packages/decord.libs/libvpx-5769b0ce.so.6.4.0\n",
            "    /usr/local/lib/python3.8/dist-packages/decord.libs/libx264-555fb44a.so.164\n",
            "    /usr/local/lib/python3.8/dist-packages/decord.libs/libxcb-77222338.so.1.1.0\n",
            "    /usr/local/lib/python3.8/dist-packages/decord.libs/libxcb-shape-893f3868.so.0.0.0\n",
            "    /usr/local/lib/python3.8/dist-packages/decord.libs/libxcb-shm-7ffb2544.so.0.0.0\n",
            "    /usr/local/lib/python3.8/dist-packages/decord.libs/libxcb-xfixes-6523fc53.so.0.0.0\n",
            "    /usr/local/lib/python3.8/dist-packages/decord.libs/libz-eb09ad1d.so.1.2.3\n",
            "    /usr/local/lib/python3.8/dist-packages/decord/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled decord-0.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting decord\n",
            "  Using cached decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from decord) (1.21.6)\n",
            "Installing collected packages: decord\n",
            "Successfully installed decord-0.6.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "decord"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: pytorch-lightning 1.8.6\n",
            "Uninstalling pytorch-lightning-1.8.6:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.8/dist-packages/lightning_lite/*\n",
            "    /usr/local/lib/python3.8/dist-packages/pytorch_lightning-1.8.6.dist-info/*\n",
            "    /usr/local/lib/python3.8/dist-packages/pytorch_lightning/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled pytorch-lightning-1.8.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-lightning\n",
            "  Using cached pytorch_lightning-1.8.6-py3-none-any.whl (800 kB)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (2022.11.0)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (4.64.1)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (0.11.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (6.0)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (1.13.1+cu116)\n",
            "Requirement already satisfied: lightning-utilities!=0.4.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (0.5.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (1.21.6)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (4.4.0)\n",
            "Requirement already satisfied: tensorboardX>=2.2 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (2.5.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.25.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=17.0->pytorch-lightning) (3.0.9)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX>=2.2->pytorch-lightning) (3.19.6)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.1.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (22.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.8.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.0)\n",
            "Installing collected packages: pytorch-lightning\n",
            "Successfully installed pytorch-lightning-1.8.6\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "lightning_lite"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "y\n",
            "Found existing installation: lightning-utilities 0.5.0\n",
            "Uninstalling lightning-utilities-0.5.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.8/dist-packages/lightning_utilities-0.5.0.dist-info/*\n",
            "    /usr/local/lib/python3.8/dist-packages/lightning_utilities/*\n",
            "Proceed (Y/n)?   Successfully uninstalled lightning-utilities-0.5.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting lightning_utilities\n",
            "  Using cached lightning_utilities-0.5.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from lightning_utilities) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from lightning_utilities) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->lightning_utilities) (3.0.9)\n",
            "y\n",
            "Installing collected packages: lightning_utilities\n",
            "Successfully installed lightning_utilities-0.5.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "lightning_utilities"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torchmetrics 0.11.0\n",
            "Uninstalling torchmetrics-0.11.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.8/dist-packages/torchmetrics-0.11.0.dist-info/*\n",
            "    /usr/local/lib/python3.8/dist-packages/torchmetrics/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled torchmetrics-0.11.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Using cached torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.11.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torchmetrics"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: tensorboardX 2.5.1\n",
            "Uninstalling tensorboardX-2.5.1:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.8/dist-packages/tensorboardX-2.5.1.dist-info/*\n",
            "    /usr/local/lib/python3.8/dist-packages/tensorboardX/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled tensorboardX-2.5.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboardX\n",
            "  Using cached tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (3.19.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (1.21.6)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.5.1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import einops\n",
        "\n",
        "from einops import rearrange, reduce, repeat\n",
        "from IPython.display import display\n",
        "\n",
        "!git clone https://github.com/mx-mark/VideoTransformer-pytorch.git\n",
        "%cd VideoTransformer-pytorch\n",
        "\n",
        "!pip uninstall decord\n",
        "!pip install decord\n",
        "!pip uninstall pytorch-lightning\n",
        "!pip install pytorch-lightning\n",
        "!pip uninstall lightning_utilities\n",
        "!pip install lightning_utilities\n",
        "!pip uninstall torchmetrics\n",
        "!pip install torchmetrics\n",
        "!pip uninstall tensorboardX\n",
        "!pip install tensorboardX\n",
        "\n",
        "import data_transform as T\n",
        "from dataset import DecordInit, load_annotation_data\n",
        "from transformer import PatchEmbed, TransformerContainer, ClassificationHead"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enw4z7HOwNIo"
      },
      "source": [
        "### Note\n",
        "Please firstly dowload the weights and move to the current path `./VideoTransformer-pytorch/`\n",
        "1. TimeSformer-B pre-trained on K400 https://drive.google.com/file/d/1jLkS24jkpmakPi3e5J8KH3FOPv370zvo/view?usp=sharing\n",
        "2. ViViT-B pre-trained on K400 from https://drive.google.com/file/d/1-JVhSN3QHKUOLkXLWXWn5drdvKn0gPll/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQLoPxX3HEa0",
        "outputId": "3144ce5a-d1c1-4254-fa66-56a925291353"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yLko87R2_m4",
        "pycharm": {}
      },
      "source": [
        "## Video Transformer Model\n",
        "\n",
        "We here load the pretrained weights of the transformer model TimeSformer-B or ViViT-B."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "d2-9Xl27wNIq"
      },
      "outputs": [],
      "source": [
        "class TimeSformer(nn.Module):\n",
        "    \"\"\"TimeSformer. A PyTorch impl of `Is Space-Time Attention All You Need for\n",
        "    Video Understanding? <https://arxiv.org/abs/2102.05095>`_\n",
        "\n",
        "    Args:\n",
        "        num_frames (int): Number of frames in the video.\n",
        "        img_size (int | tuple): Size of input image.\n",
        "        patch_size (int): Size of one patch.\n",
        "        pretrained (str | None): Name of pretrained model. Default: None.\n",
        "        embed_dims (int): Dimensions of embedding. Defaults to 768.\n",
        "        num_heads (int): Number of parallel attention heads in\n",
        "            TransformerCoder. Defaults to 12.\n",
        "        num_transformer_layers (int): Number of transformer layers. Defaults to\n",
        "            12.\n",
        "        in_channels (int): Channel num of input features. Defaults to 3.\n",
        "        dropout_p (float): Probability of dropout layer. Defaults to 0.\n",
        "        conv_type (str): Type of the convolution in PatchEmbed layer. Defaults to Conv2d.\n",
        "        attention_type (str): Type of attentions in TransformerCoder. Choices\n",
        "            are 'divided_space_time', 'space_only' and 'joint_space_time'.\n",
        "            Defaults to 'divided_space_time'.\n",
        "        norm_layer (dict): Config for norm layers. Defaults to nn.LayerNorm.\n",
        "        return_cls_token (bool): Whether to use cls_token to predict class label.\n",
        "    \"\"\"\n",
        "    supported_attention_types = [\n",
        "        'divided_space_time', 'space_only', 'joint_space_time'\n",
        "    ]\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_frames,\n",
        "                 img_size,\n",
        "                 patch_size,\n",
        "                 embed_dims=768,\n",
        "                 num_heads=12,\n",
        "                 num_transformer_layers=12,\n",
        "                 in_channels=3,\n",
        "                 conv_type='Conv2d',\n",
        "                 dropout_p=0.,\n",
        "                 attention_type='divided_space_time',\n",
        "                 norm_layer=nn.LayerNorm,\n",
        "                 return_cls_token=True,\n",
        "                 **kwargs):\n",
        "        super().__init__()\n",
        "        assert attention_type in self.supported_attention_types, (\n",
        "            f'Unsupported Attention Type {attention_type}!')\n",
        "\n",
        "        self.num_frames = num_frames\n",
        "        self.embed_dims = embed_dims\n",
        "        self.num_transformer_layers = num_transformer_layers\n",
        "        self.attention_type = attention_type\n",
        "        self.conv_type = conv_type\n",
        "        self.return_cls_token = return_cls_token\n",
        "\n",
        "        #tokenize & position embedding\n",
        "        self.patch_embed = PatchEmbed(\n",
        "            img_size=img_size,\n",
        "            patch_size=patch_size,\n",
        "            in_channels=in_channels,\n",
        "            embed_dims=embed_dims,\n",
        "            conv_type=conv_type)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        # Divided Space Time Attention\n",
        "        operator_order = ['time_attn','space_attn','ffn']\n",
        "        container = TransformerContainer(\n",
        "            num_transformer_layers=num_transformer_layers,\n",
        "            embed_dims=embed_dims,\n",
        "            num_heads=num_heads,\n",
        "            num_frames=num_frames,\n",
        "            norm_layer=norm_layer,\n",
        "            hidden_channels=embed_dims*4,\n",
        "            operator_order=operator_order)\n",
        "\n",
        "        self.transformer_layers = container\n",
        "        self.norm = norm_layer(embed_dims, eps=1e-6)\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dims))\n",
        "        num_patches = num_patches + 1\n",
        "\n",
        "        # spatial pos_emb\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1,num_patches,embed_dims))\n",
        "        self.drop_after_pos = nn.Dropout(p=dropout_p)\n",
        "\n",
        "        # temporal pos_emb\n",
        "        self.time_embed = nn.Parameter(torch.zeros(1,num_frames,embed_dims))\n",
        "        self.drop_after_time = nn.Dropout(p=dropout_p)\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, w, h):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = self.pos_embed.shape[1] - 1\n",
        "        if npatch == N and w == h:\n",
        "            return self.pos_embed\n",
        "        class_pos_embed = self.pos_embed[:, 0]\n",
        "        patch_pos_embed = self.pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        w0 = w // self.patch_embed.patch_size[0]\n",
        "        h0 = h // self.patch_embed.patch_size[0]\n",
        "        # we add a small number to avoid floating point error in the interpolation\n",
        "        # see discussion at https://github.com/facebookresearch/dino/issues/8\n",
        "        w0, h0 = w0 + 0.1, h0 + 0.1\n",
        "        patch_pos_embed = nn.functional.interpolate(\n",
        "            patch_pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
        "            scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n",
        "            mode='bicubic',\n",
        "        )\n",
        "        assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]\n",
        "        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #Tokenize\n",
        "        b, t, c, h, w = x.shape\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        # Add Position Embedding\n",
        "        cls_tokens = repeat(self.cls_token, 'b ... -> (repeat b) ...', repeat=x.shape[0])\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.interpolate_pos_encoding(x, w, h) #self.pos_embed\n",
        "        x = self.drop_after_pos(x)\n",
        "\n",
        "        # Add Time Embedding\n",
        "        cls_tokens = x[:b, 0, :].unsqueeze(1)\n",
        "        x = rearrange(x[:, 1:, :], '(b t) p d -> (b p) t d', b=b)\n",
        "        x = x + self.time_embed\n",
        "        x = rearrange(x, '(b p) t d -> b (p t) d', b=b)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = self.drop_after_time(x)\n",
        "\n",
        "        # Video transformer forward\n",
        "        x = self.transformer_layers(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        # Return Class Token\n",
        "        if self.return_cls_token:\n",
        "            return x[:, 0]\n",
        "        else:\n",
        "            return x[:, 1:].mean(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TEdeVIyewNIs"
      },
      "outputs": [],
      "source": [
        "class ViViT(nn.Module):\n",
        "    \"\"\"ViViT. A PyTorch impl of `ViViT: A Video Vision Transformer`\n",
        "        <https://arxiv.org/abs/2103.15691>\n",
        "\n",
        "    Args:\n",
        "        num_frames (int): Number of frames in the video.\n",
        "        img_size (int | tuple): Size of input image.\n",
        "        patch_size (int): Size of one patch.\n",
        "        pretrained (str | None): Name of pretrained model. Default: None.\n",
        "        embed_dims (int): Dimensions of embedding. Defaults to 768.\n",
        "        num_heads (int): Number of parallel attention heads. Defaults to 12.\n",
        "        num_transformer_layers (int): Number of transformer layers. Defaults to 12.\n",
        "        in_channels (int): Channel num of input features. Defaults to 3.\n",
        "        dropout_p (float): Probability of dropout layer. Defaults to 0..\n",
        "        tube_size (int): Dimension of the kernel size in Conv3d. Defaults to 2.\n",
        "        conv_type (str): Type of the convolution in PatchEmbed layer. Defaults to Conv3d.\n",
        "        attention_type (str): Type of attentions in TransformerCoder. Choices\n",
        "            are 'divided_space_time', 'fact_encoder' and 'joint_space_time'.\n",
        "            Defaults to 'fact_encoder'.\n",
        "        norm_layer (dict): Config for norm layers. Defaults to nn.LayerNorm.\n",
        "        copy_strategy (str): Copy or Initial to zero towards the new additional layer.\n",
        "        extend_strategy (str): How to initialize the weights of Conv3d from pre-trained Conv2d.\n",
        "        use_learnable_pos_emb (bool): Whether to use learnable position embeddings.\n",
        "        return_cls_token (bool): Whether to use cls_token to predict class label.\n",
        "    \"\"\"\n",
        "    supported_attention_types = [\n",
        "        'fact_encoder', 'joint_space_time', 'divided_space_time'\n",
        "    ]\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_frames,\n",
        "                 img_size,\n",
        "                 patch_size,\n",
        "                 embed_dims=768,\n",
        "                 num_heads=12,\n",
        "                 num_transformer_layers=12,\n",
        "                 in_channels=3,\n",
        "                 dropout_p=0.,\n",
        "                 tube_size=2,\n",
        "                 conv_type='Conv3d',\n",
        "                 attention_type='fact_encoder',\n",
        "                 norm_layer=nn.LayerNorm,\n",
        "                 return_cls_token=True,\n",
        "                 **kwargs):\n",
        "        super().__init__()\n",
        "        assert attention_type in self.supported_attention_types, (\n",
        "            f'Unsupported Attention Type {attention_type}!')\n",
        "\n",
        "        num_frames = num_frames//tube_size\n",
        "        self.num_frames = num_frames\n",
        "        self.embed_dims = embed_dims\n",
        "        self.num_transformer_layers = num_transformer_layers\n",
        "        self.attention_type = attention_type\n",
        "        self.conv_type = conv_type\n",
        "        self.tube_size = tube_size\n",
        "        self.num_time_transformer_layers = 4\n",
        "        self.return_cls_token = return_cls_token\n",
        "\n",
        "        #tokenize & position embedding\n",
        "        self.patch_embed = PatchEmbed(\n",
        "            img_size=img_size,\n",
        "            patch_size=patch_size,\n",
        "            in_channels=in_channels,\n",
        "            embed_dims=embed_dims,\n",
        "            tube_size=tube_size,\n",
        "            conv_type=conv_type)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        # Divided Space Time Transformer Encoder - Model 2\n",
        "        transformer_layers = nn.ModuleList([])\n",
        "\n",
        "        spatial_transformer = TransformerContainer(\n",
        "            num_transformer_layers=num_transformer_layers,\n",
        "            embed_dims=embed_dims,\n",
        "            num_heads=num_heads,\n",
        "            num_frames=num_frames,\n",
        "            norm_layer=norm_layer,\n",
        "            hidden_channels=embed_dims*4,\n",
        "            operator_order=['self_attn','ffn'])\n",
        "\n",
        "        temporal_transformer = TransformerContainer(\n",
        "            num_transformer_layers=self.num_time_transformer_layers,\n",
        "            embed_dims=embed_dims,\n",
        "            num_heads=num_heads,\n",
        "            num_frames=num_frames,\n",
        "            norm_layer=norm_layer,\n",
        "            hidden_channels=embed_dims*4,\n",
        "            operator_order=['self_attn','ffn'])\n",
        "\n",
        "        transformer_layers.append(spatial_transformer)\n",
        "        transformer_layers.append(temporal_transformer)\n",
        "\n",
        "        self.transformer_layers = transformer_layers\n",
        "        self.norm = norm_layer(embed_dims, eps=1e-6)\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dims))\n",
        "        # whether to add one cls_token in temporal pos_enb\n",
        "        num_frames = num_frames + 1\n",
        "        num_patches = num_patches + 1\n",
        "\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1,num_patches,embed_dims))\n",
        "        self.time_embed = nn.Parameter(torch.zeros(1,num_frames,embed_dims))\n",
        "        self.drop_after_pos = nn.Dropout(p=dropout_p)\n",
        "        self.drop_after_time = nn.Dropout(p=dropout_p)\n",
        "    \n",
        "    def interpolate_pos_encoding(self, x, w, h):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = self.pos_embed.shape[1] - 1\n",
        "        if npatch == N and w == h:\n",
        "            return self.pos_embed\n",
        "        class_pos_embed = self.pos_embed[:, 0]\n",
        "        patch_pos_embed = self.pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        w0 = w // self.patch_embed.patch_size[0]\n",
        "        h0 = h // self.patch_embed.patch_size[0]\n",
        "        # we add a small number to avoid floating point error in the interpolation\n",
        "        # see discussion at https://github.com/facebookresearch/dino/issues/8\n",
        "        w0, h0 = w0 + 0.1, h0 + 0.1\n",
        "        patch_pos_embed = nn.functional.interpolate(\n",
        "            patch_pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
        "            scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n",
        "            mode='bicubic',\n",
        "        )\n",
        "        assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]\n",
        "        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #Tokenize\n",
        "        b, t, c, h, w = x.shape\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        # Add Position Embedding\n",
        "        cls_tokens = repeat(self.cls_token, 'b ... -> (repeat b) ...', repeat=x.shape[0])\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.interpolate_pos_encoding(x, w, h)\n",
        "        x = self.drop_after_pos(x)\n",
        "\n",
        "        # fact encoder - CRNN style\n",
        "        spatial_transformer, temporal_transformer, = *self.transformer_layers,\n",
        "        x = spatial_transformer(x)\n",
        "\n",
        "        # Add Time Embedding\n",
        "        cls_tokens = x[:b, 0, :].unsqueeze(1)\n",
        "        x = rearrange(x[:, 1:, :], '(b t) p d -> b t p d', b=b)\n",
        "        x = reduce(x, 'b t p d -> b t d', 'mean')\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.time_embed\n",
        "        x = self.drop_after_time(x)\n",
        "\n",
        "        x = temporal_transformer(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        # Return Class Token\n",
        "        if self.return_cls_token:\n",
        "            return x[:, 0]\n",
        "        else:\n",
        "            return x[:, 1:].mean(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yv9yVGoXwNIu"
      },
      "outputs": [],
      "source": [
        "def replace_state_dict(state_dict):\n",
        "\tfor old_key in list(state_dict.keys()):\n",
        "\t\tif old_key.startswith('model'):\n",
        "\t\t\tnew_key = old_key[6:]\n",
        "\t\t\tstate_dict[new_key] = state_dict.pop(old_key)\n",
        "\t\telse:\n",
        "\t\t\tnew_key = old_key[9:]\n",
        "\t\t\tstate_dict[new_key] = state_dict.pop(old_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "g8MQPG6KwNIv"
      },
      "outputs": [],
      "source": [
        "def init_from_pretrain_(module, pretrained, init_module):\n",
        "    if torch.cuda.is_available():\n",
        "        state_dict = torch.load(pretrained)\n",
        "    else:\n",
        "        state_dict = torch.load(pretrained, map_location=torch.device('cpu'))\n",
        "    if init_module == 'transformer':\n",
        "        replace_state_dict(state_dict)\n",
        "    elif init_module == 'cls_head':\n",
        "        replace_state_dict(state_dict)\n",
        "    else:\n",
        "        raise TypeError(f'pretrained weights do not include the {init_module} module')\n",
        "    msg = module.load_state_dict(state_dict, strict=False)\n",
        "    return msg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5J7lGPJ2bLJ",
        "outputId": "8234517a-c17d-4d92-91f7-08193ff0380a",
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load model finished, the missing key of transformer is:['transformer_layers.0.layers.0.attentions.0.attn.qkv.weight', 'transformer_layers.0.layers.0.attentions.0.attn.qkv.bias', 'transformer_layers.0.layers.0.attentions.0.attn.proj.weight', 'transformer_layers.0.layers.0.attentions.0.attn.proj.bias', 'transformer_layers.0.layers.1.attentions.0.attn.qkv.weight', 'transformer_layers.0.layers.1.attentions.0.attn.qkv.bias', 'transformer_layers.0.layers.1.attentions.0.attn.proj.weight', 'transformer_layers.0.layers.1.attentions.0.attn.proj.bias', 'transformer_layers.0.layers.2.attentions.0.attn.qkv.weight', 'transformer_layers.0.layers.2.attentions.0.attn.qkv.bias', 'transformer_layers.0.layers.2.attentions.0.attn.proj.weight', 'transformer_layers.0.layers.2.attentions.0.attn.proj.bias', 'transformer_layers.0.layers.3.attentions.0.attn.qkv.weight', 'transformer_layers.0.layers.3.attentions.0.attn.qkv.bias', 'transformer_layers.0.layers.3.attentions.0.attn.proj.weight', 'transformer_layers.0.layers.3.attentions.0.attn.proj.bias', 'transformer_layers.0.layers.4.attentions.0.attn.qkv.weight', 'transformer_layers.0.layers.4.attentions.0.attn.qkv.bias', 'transformer_layers.0.layers.4.attentions.0.attn.proj.weight', 'transformer_layers.0.layers.4.attentions.0.attn.proj.bias', 'transformer_layers.0.layers.5.attentions.0.attn.qkv.weight', 'transformer_layers.0.layers.5.attentions.0.attn.qkv.bias', 'transformer_layers.0.layers.5.attentions.0.attn.proj.weight', 'transformer_layers.0.layers.5.attentions.0.attn.proj.bias', 'transformer_layers.0.layers.6.attentions.0.attn.qkv.weight', 'transformer_layers.0.layers.6.attentions.0.attn.qkv.bias', 'transformer_layers.0.layers.6.attentions.0.attn.proj.weight', 'transformer_layers.0.layers.6.attentions.0.attn.proj.bias', 'transformer_layers.0.layers.7.attentions.0.attn.qkv.weight', 'transformer_layers.0.layers.7.attentions.0.attn.qkv.bias', 'transformer_layers.0.layers.7.attentions.0.attn.proj.weight', 'transformer_layers.0.layers.7.attentions.0.attn.proj.bias', 'transformer_layers.0.layers.8.attentions.0.attn.qkv.weight', 'transformer_layers.0.layers.8.attentions.0.attn.qkv.bias', 'transformer_layers.0.layers.8.attentions.0.attn.proj.weight', 'transformer_layers.0.layers.8.attentions.0.attn.proj.bias', 'transformer_layers.0.layers.9.attentions.0.attn.qkv.weight', 'transformer_layers.0.layers.9.attentions.0.attn.qkv.bias', 'transformer_layers.0.layers.9.attentions.0.attn.proj.weight', 'transformer_layers.0.layers.9.attentions.0.attn.proj.bias', 'transformer_layers.0.layers.10.attentions.0.attn.qkv.weight', 'transformer_layers.0.layers.10.attentions.0.attn.qkv.bias', 'transformer_layers.0.layers.10.attentions.0.attn.proj.weight', 'transformer_layers.0.layers.10.attentions.0.attn.proj.bias', 'transformer_layers.0.layers.11.attentions.0.attn.qkv.weight', 'transformer_layers.0.layers.11.attentions.0.attn.qkv.bias', 'transformer_layers.0.layers.11.attentions.0.attn.proj.weight', 'transformer_layers.0.layers.11.attentions.0.attn.proj.bias', 'transformer_layers.1.layers.0.attentions.0.attn.qkv.weight', 'transformer_layers.1.layers.0.attentions.0.attn.qkv.bias', 'transformer_layers.1.layers.0.attentions.0.attn.proj.weight', 'transformer_layers.1.layers.0.attentions.0.attn.proj.bias', 'transformer_layers.1.layers.1.attentions.0.attn.qkv.weight', 'transformer_layers.1.layers.1.attentions.0.attn.qkv.bias', 'transformer_layers.1.layers.1.attentions.0.attn.proj.weight', 'transformer_layers.1.layers.1.attentions.0.attn.proj.bias', 'transformer_layers.1.layers.2.attentions.0.attn.qkv.weight', 'transformer_layers.1.layers.2.attentions.0.attn.qkv.bias', 'transformer_layers.1.layers.2.attentions.0.attn.proj.weight', 'transformer_layers.1.layers.2.attentions.0.attn.proj.bias', 'transformer_layers.1.layers.3.attentions.0.attn.qkv.weight', 'transformer_layers.1.layers.3.attentions.0.attn.qkv.bias', 'transformer_layers.1.layers.3.attentions.0.attn.proj.weight', 'transformer_layers.1.layers.3.attentions.0.attn.proj.bias'], cls is:[]\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
        "num_frames = 8\n",
        "frame_interval = 32\n",
        "num_class = 400\n",
        "arch = 'vivit' # turn to vivit for initializing vivit model\n",
        "\n",
        "if arch == 'timesformer':\n",
        "    pretrain_pth = '/content/drive/My Drive/vivit_model.pth'\n",
        "    model = TimeSformer(num_frames=num_frames,\n",
        "                        img_size=224,\n",
        "                        patch_size=16,\n",
        "                        embed_dims=768,\n",
        "                        in_channels=3,\n",
        "                        attention_type='divided_space_time',\n",
        "                        return_cls_token=True)\n",
        "elif arch == 'vivit':\n",
        "    pretrain_pth = '/content/drive/My Drive/vivit_model.pth'\n",
        "    num_frames = num_frames * 2\n",
        "    frame_interval = frame_interval // 2\n",
        "    model = ViViT(num_frames=num_frames,\n",
        "                  img_size=224,\n",
        "                  patch_size=16,\n",
        "                  embed_dims=768,\n",
        "                  in_channels=3,\n",
        "                  attention_type='fact_encoder',\n",
        "                  return_cls_token=True)\n",
        "else:\n",
        "    raise TypeError(f'not supported arch type {arch}, chosen in (timesformer, vivit)')\n",
        "\n",
        "cls_head = ClassificationHead(num_classes=num_class, in_channels=768)\n",
        "msg_trans = init_from_pretrain_(model, pretrain_pth, init_module='transformer')\n",
        "msg_cls = init_from_pretrain_(cls_head, pretrain_pth, init_module='cls_head')\n",
        "model.eval()\n",
        "cls_head.eval()\n",
        "model = model.to(device)\n",
        "cls_head = cls_head.to(device)\n",
        "print(f'load model finished, the missing key of transformer is:{msg_trans[0]}, cls is:{msg_cls[0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He3miBT7wNIx"
      },
      "source": [
        "## Data preprocess\n",
        "\n",
        "Here we show the video demo and transform the video input for the model processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502,
          "resources": {
            "http://localhost:8080/demo/YABnJL_bDzw.mp4": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "ok": false,
              "status": 404,
              "status_text": ""
            }
          }
        },
        "id": "7nTvEpzWwNIy",
        "outputId": "c0810bb0-77f1-4ee7-9ebf-ecce60acbc74"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<video controls width=\"480\" height=\"480\" src=\"./demo/YABnJL_bDzw.mp4\">animation</video>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import display, HTML\n",
        "\n",
        "video_path = './demo/YABnJL_bDzw.mp4'\n",
        "html_str = '''\n",
        "<video controls width=\\\"480\\\" height=\\\"480\\\" src=\\\"{}\\\">animation</video>\n",
        "'''.format(video_path)\n",
        "display(HTML(html_str))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "EC0KomqHwNIy"
      },
      "outputs": [],
      "source": [
        "# Prepare data preprocess\n",
        "mean, std = (0.45, 0.45, 0.45), (0.225, 0.225, 0.225)\n",
        "data_transform = T.Compose([\n",
        "        T.Resize(scale_range=(-1, 256)),\n",
        "        T.ThreeCrop(size=224),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean, std)\n",
        "        ])\n",
        "temporal_sample = T.TemporalRandomCrop(num_frames*frame_interval)\n",
        "\n",
        "# Sampling video frames\n",
        "video_decoder = DecordInit()\n",
        "v_reader = video_decoder(video_path)\n",
        "total_frames = len(v_reader)\n",
        "start_frame_ind, end_frame_ind = temporal_sample(total_frames)\n",
        "if end_frame_ind-start_frame_ind < num_frames:\n",
        "    raise ValueError(f'the total frames of the video {video_path} is less than {num_frames}')\n",
        "frame_indice = np.linspace(0, end_frame_ind-start_frame_ind-1, num_frames, dtype=int)\n",
        "video = v_reader.get_batch(frame_indice).asnumpy()\n",
        "del v_reader\n",
        "\n",
        "video = torch.from_numpy(video).permute(0,3,1,2) # Video transform: T C H W\n",
        "data_transform.randomize_parameters()\n",
        "video = data_transform(video)\n",
        "video = video.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHiNWesbwNIz"
      },
      "source": [
        "## Video Classification\n",
        "\n",
        "Here we use the pre-trained video transformer to classify the input video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iK9vi09wNIz",
        "outputId": "e0f71247-35eb-4245-e753-d373721a20fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the shape of ouptut: torch.Size([400]), and the prediction is: finger_snapping\n"
          ]
        }
      ],
      "source": [
        "# Predict class label\n",
        "with torch.no_grad():\n",
        "    logits = model(video)\n",
        "    output = cls_head(logits)\n",
        "    output = output.view(3, 400).mean(0)\n",
        "    cls_pred = output.argmax().item()\n",
        "\n",
        "class_map = './k400_classmap.json'\n",
        "class_map = load_annotation_data(class_map)\n",
        "for key, value in class_map.items():\n",
        "    if int(value) == int(cls_pred):\n",
        "        print(f'the shape of ouptut: {output.shape}, and the prediction is: {key}')\n",
        "        break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "iBOT_demo",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.1 (v3.10.1:2cd268a3a9, Dec  6 2021, 14:28:59) [Clang 13.0.0 (clang-1300.0.29.3)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
