{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "regulation-reader",
      "metadata": {
        "id": "regulation-reader"
      },
      "source": [
        "### This notebook is optionally accelerated with a GPU runtime.\n",
        "### If you would like to use this acceleration, please select the menu option \"Runtime\" -> \"Change runtime type\", select \"Hardware Accelerator\" -> \"GPU\" and click \"SAVE\"\n",
        "\n",
        "----------------------------------------------------------------------\n",
        "\n",
        "# YOLOP\n",
        "\n",
        "*Author: Hust Visual Learning Team*\n",
        "\n",
        "**YOLOP pretrained on the BDD100K dataset**\n",
        "\n",
        "## Before You Start\n",
        "To install YOLOP dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "accomplished-batman",
      "metadata": {
        "id": "accomplished-batman"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Invalid requirement: '<!DOCTYPE html>' (from line 8 of https://github.com/hustvl/YOLOP/blob/main/requirements.txt)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "! pip install -qr https://github.com/hustvl/YOLOP/blob/main/requirements.txt  # install dependencies"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "junior-sheep",
      "metadata": {
        "id": "junior-sheep"
      },
      "source": [
        "## YOLOP: You Only Look Once for Panoptic driving Perception (paramonic)\n",
        "\n",
        "### Model Description\n",
        "\n",
        "<img width=\"800\" alt=\"YOLOP Model\" src=\"https://github.com/hustvl/YOLOP/raw/main/pictures/yolop.png\">\n",
        "&nbsp;\n",
        "\n",
        "- YOLOP is an efficient multi-task network that can jointly handle three crucial tasks in autonomous driving: object detection, drivable area segmentation and lane detection. And it is also the first to reach real-time on embedded devices while maintaining state-of-the-art level performance on the **BDD100K** dataset.\n",
        "\n",
        "\n",
        "### Results\n",
        "\n",
        "#### Traffic Object Detection Result\n",
        "\n",
        "| Model          | Recall(%) | mAP50(%) | Speed(fps) |\n",
        "| -------------- | --------- | -------- | ---------- |\n",
        "| `Multinet`     | 81.3      | 60.2     | 8.6        |\n",
        "| `DLT-Net`      | 89.4      | 68.4     | 9.3        |\n",
        "| `Faster R-CNN` | 77.2      | 55.6     | 5.3        |\n",
        "| `YOLOv5s`      | 86.8      | 77.2     | 82         |\n",
        "| `YOLOP(ours)`  | 89.2      | 76.5     | 41         |\n",
        "\n",
        "#### Drivable Area Segmentation Result\n",
        "\n",
        "| Model         | mIOU(%) | Speed(fps) |\n",
        "| ------------- | ------- | ---------- |\n",
        "| `Multinet`    | 71.6    | 8.6        |\n",
        "| `DLT-Net`     | 71.3    | 9.3        |\n",
        "| `PSPNet`      | 89.6    | 11.1       |\n",
        "| `YOLOP(ours)` | 91.5    | 41         |\n",
        "\n",
        "#### Lane Detection Result\n",
        "\n",
        "| Model         | mIOU(%) | IOU(%) |\n",
        "| ------------- | ------- | ------ |\n",
        "| `ENet`        | 34.12   | 14.64  |\n",
        "| `SCNN`        | 35.79   | 15.84  |\n",
        "| `ENet-SAD`    | 36.56   | 16.02  |\n",
        "| `YOLOP(ours)` | 70.50   | 26.20  |\n",
        "\n",
        "#### Ablation Studies 1: End-to-end v.s. Step-by-step\n",
        "\n",
        "| Training_method | Recall(%) | AP(%) | mIoU(%) | Accuracy(%) | IoU(%) |\n",
        "| --------------- | --------- | ----- | ------- | ----------- | ------ |\n",
        "| `ES-W`          | 87.0      | 75.3  | 90.4    | 66.8        | 26.2   |\n",
        "| `ED-W`          | 87.3      | 76.0  | 91.6    | 71.2        | 26.1   |\n",
        "| `ES-D-W`        | 87.0      | 75.1  | 91.7    | 68.6        | 27.0   |\n",
        "| `ED-S-W`        | 87.5      | 76.1  | 91.6    | 68.0        | 26.8   |\n",
        "| `End-to-end`    | 89.2      | 76.5  | 91.5    | 70.5        | 26.2   |\n",
        "\n",
        "#### Ablation Studies 2: Multi-task v.s. Single task\n",
        "\n",
        "| Training_method | Recall(%) | AP(%) | mIoU(%) | Accuracy(%) | IoU(%) | Speed(ms/frame) |\n",
        "| --------------- | --------- | ----- | ------- | ----------- | ------ | --------------- |\n",
        "| `Det(only)`     | 88.2      | 76.9  | -       | -           | -      | 15.7            |\n",
        "| `Da-Seg(only)`  | -         | -     | 92.0    | -           | -      | 14.8            |\n",
        "| `Ll-Seg(only)`  | -         | -     | -       | 79.6        | 27.9   | 14.8            |\n",
        "| `Multitask`     | 89.2      | 76.5  | 91.5    | 70.5        | 26.2   | 24.4            |\n",
        "\n",
        "**Notes**:\n",
        "\n",
        "- In table 4, E, D, S and W refer to Encoder, Detect head, two Segment heads and whole network. So the Algorithm (First, we only train Encoder and Detect head. Then we freeze the Encoder and Detect head as well as train two Segmentation heads. Finally, the entire network is trained jointly for all three tasks.) can be marked as ED-S-W, and the same for others.\n",
        "\n",
        "### Visualization\n",
        "\n",
        "#### Traffic Object Detection Result\n",
        "\n",
        "<img width=\"800\" alt=\"Traffic Object Detection Result\" src=\"https://github.com/hustvl/YOLOP/raw/main/pictures/detect.png\">\n",
        "&nbsp;\n",
        "\n",
        "#### Drivable Area Segmentation Result\n",
        "\n",
        "<img width=\"800\" alt=\"Drivable Area Segmentation Result\" src=\"https://github.com/hustvl/YOLOP/raw/main/pictures/da.png\">\n",
        "&nbsp;\n",
        "\n",
        "#### Lane Detection Result\n",
        "\n",
        "<img width=\"800\" alt=\"Lane Detection Result\" src=\"https://github.com/hustvl/YOLOP/raw/main/pictures/ll.png\">\n",
        "&nbsp;\n",
        "\n",
        "**Notes**:\n",
        "\n",
        "- The visualization of lane detection result has been post processed by quadratic fitting.\n",
        "\n",
        "### Deployment\n",
        "\n",
        "Our model can reason in real-time on **Jetson Tx2**, with **Zed Camera** to capture image. We use **TensorRT** tool for speeding up. We provide code for deployment and reasoning of model in [github code](https://github.com/hustvl/YOLOP/tree/main/toolkits/deploy).\n",
        "\n",
        "\n",
        "### Load From PyTorch Hub\n",
        "This example loads the pretrained **YOLOP** model and passes an image for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "polish-theology",
      "metadata": {
        "id": "polish-theology"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/hustvl/yolop/archive/main.zip\" to /Users/kimminchae/.cache/torch/hub/main.zip\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/kimminchae/iCloud Drive(아카이브)/Desktop/CNN/CNN/yolop.ipynb 셀 4\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kimminchae/iCloud%C2%A0Drive%28%EC%95%84%EC%B9%B4%EC%9D%B4%EB%B8%8C%29/Desktop/CNN/CNN/yolop.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kimminchae/iCloud%C2%A0Drive%28%EC%95%84%EC%B9%B4%EC%9D%B4%EB%B8%8C%29/Desktop/CNN/CNN/yolop.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# load model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kimminchae/iCloud%C2%A0Drive%28%EC%95%84%EC%B9%B4%EC%9D%B4%EB%B8%8C%29/Desktop/CNN/CNN/yolop.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mhub\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mhustvl/yolop\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39myolop\u001b[39;49m\u001b[39m'\u001b[39;49m, pretrained\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kimminchae/iCloud%C2%A0Drive%28%EC%95%84%EC%B9%B4%EC%9D%B4%EB%B8%8C%29/Desktop/CNN/CNN/yolop.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#inference\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kimminchae/iCloud%C2%A0Drive%28%EC%95%84%EC%B9%B4%EC%9D%B4%EB%B8%8C%29/Desktop/CNN/CNN/yolop.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m640\u001b[39m,\u001b[39m640\u001b[39m)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/hub.py:402\u001b[0m, in \u001b[0;36mload\u001b[0;34m(repo_or_dir, model, source, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    399\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mUnknown source: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00msource\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m. Allowed values: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgithub\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m | \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlocal\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m source \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgithub\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 402\u001b[0m     repo_or_dir \u001b[39m=\u001b[39m _get_cache_or_reload(repo_or_dir, force_reload, verbose, skip_validation)\n\u001b[1;32m    404\u001b[0m model \u001b[39m=\u001b[39m _load_local(repo_or_dir, model, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    405\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/hub.py:197\u001b[0m, in \u001b[0;36m_get_cache_or_reload\u001b[0;34m(github, force_reload, verbose, skip_validation)\u001b[0m\n\u001b[1;32m    195\u001b[0m url \u001b[39m=\u001b[39m _git_archive_link(repo_owner, repo_name, branch)\n\u001b[1;32m    196\u001b[0m sys\u001b[39m.\u001b[39mstderr\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39mDownloading: \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\\"\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(url, cached_file))\n\u001b[0;32m--> 197\u001b[0m download_url_to_file(url, cached_file, progress\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    199\u001b[0m \u001b[39mwith\u001b[39;00m zipfile\u001b[39m.\u001b[39mZipFile(cached_file) \u001b[39mas\u001b[39;00m cached_zipfile:\n\u001b[1;32m    200\u001b[0m     extraced_repo_name \u001b[39m=\u001b[39m cached_zipfile\u001b[39m.\u001b[39minfolist()[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mfilename\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/hub.py:479\u001b[0m, in \u001b[0;36mdownload_url_to_file\u001b[0;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(total\u001b[39m=\u001b[39mfile_size, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m progress,\n\u001b[1;32m    477\u001b[0m           unit\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mB\u001b[39m\u001b[39m'\u001b[39m, unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, unit_divisor\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m) \u001b[39mas\u001b[39;00m pbar:\n\u001b[1;32m    478\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 479\u001b[0m         buffer \u001b[39m=\u001b[39m u\u001b[39m.\u001b[39;49mread(\u001b[39m8192\u001b[39;49m)\n\u001b[1;32m    480\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(buffer) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    481\u001b[0m             \u001b[39mbreak\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:459\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    458\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunked:\n\u001b[0;32m--> 459\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_chunked(amt)\n\u001b[1;32m    461\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[1;32m    463\u001b[0m         \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:582\u001b[0m, in \u001b[0;36mHTTPResponse._read_chunked\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    581\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 582\u001b[0m         chunk_left \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_chunk_left()\n\u001b[1;32m    583\u001b[0m         \u001b[39mif\u001b[39;00m chunk_left \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    584\u001b[0m             \u001b[39mbreak\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:565\u001b[0m, in \u001b[0;36mHTTPResponse._get_chunk_left\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_safe_read(\u001b[39m2\u001b[39m)  \u001b[39m# toss the CRLF at the end of the chunk\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 565\u001b[0m     chunk_left \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_next_chunk_size()\n\u001b[1;32m    566\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m    567\u001b[0m     \u001b[39mraise\u001b[39;00m IncompleteRead(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:525\u001b[0m, in \u001b[0;36mHTTPResponse._read_next_chunk_size\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_next_chunk_size\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    524\u001b[0m     \u001b[39m# Read the next chunk size from the file\u001b[39;00m\n\u001b[0;32m--> 525\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[1;32m    526\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    527\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mchunk size\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1273\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1269\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1270\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1271\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1272\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1273\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1274\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1275\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1129\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1128\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1129\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1130\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1131\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# load model\n",
        "model = torch.hub.load('hustvl/yolop', 'yolop', pretrained=True)\n",
        "\n",
        "#inference\n",
        "img = torch.randn(1,3,640,640)\n",
        "det_out, da_seg_out,ll_seg_out = model(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pressing-glenn",
      "metadata": {
        "id": "pressing-glenn"
      },
      "source": [
        "### Citation\n",
        "\n",
        "See for more detail in [github code](https://github.com/hustvl/YOLOP) and [arxiv paper](https://arxiv.org/abs/2108.11250).\n",
        "\n",
        "If you find our paper and code useful for your research, please consider giving a star and citation:"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
